<!--
Linear Algebra Matrix Calculator
Copyright ¬© 2025 Travis Reinart

Licensed under the Creative Commons Attribution-NoDerivatives 4.0 International License.
You may copy and redistribute this work in any medium or format for any purpose,
even commercially, as long as you give appropriate credit and do not distribute modified versions.
See: https://creativecommons.org/licenses/by-nd/4.0/

Related Profiles:
LinkedIn: https://www.linkedin.com/in/travis-reinart/
GitHub: https://github.com/treinart
-->

<!DOCTYPE html>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="google-site-verification" content="google2fae6923dc85e87d" />

<title>Interactive Matrix Calculator | Step-by-Step Linear Algebra</title>
<meta name="description" content="Master linear algebra with the free ultimate step-by-step matrix calculator. Solve for eigenvalues, inverse matrices, Hessian Matrix, RREF, and more. Get detailed, educational explanations for over 50 operations. Perfect web-based educational tool for students and self-learners.">
<meta name="keywords" content="matrix calculator, linear algebra, eigenvalues, eigenvectors, hessian matrix, dot product, SVD, transpose, inverse matrix, educational math tool, cramer's rule, rref calculator">
<link rel="canonical" href="https://treinart.github.io/Linear-Algebra-Matrix-Calculator/">

<meta property="og:title" content="Interactive Matrix Calculator | Step-by-Step Linear Algebra">
<meta property="og:description" content="A free, educational tool that provides detailed, step-by-step solutions for over 50 linear algebra operations.">
<meta property="og:image" content="https://github.com/treinart/Linear-Algebra-Matrix-Calculator/blob/main/preview-image.jpg.png?raw=true">
<meta property="og:url" content="https://treinart.github.io/Linear-Algebra-Matrix-Calculator/">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="Interactive Matrix Calculator | Step-by-Step Linear Algebra">
<meta name="twitter:description" content="A free, educational tool that provides detailed, step-by-step solutions for over 50 linear algebra operations.">
<meta name="twitter:image" content="https://github.com/treinart/Linear-Algebra-Matrix-Calculator/blob/main/preview-image.jpg.png?raw=true">

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "SoftwareApplication",
    "name": "Interactive Matrix Calculator | Step-by-Step Linear Algebra",
    "operatingSystem": "WEB",
    "applicationCategory": "EducationalApplication",
    "aggregateRating": {
    "@type": "AggregateRating",
    "ratingValue": "4.9",
    "ratingCount": "215"
    },
    "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
    },
    "description": "Master linear algebra with the free ultimate step-by-step matrix calculator. Solve for eigenvalues, inverse matrices, RREF, and more. Get detailed, educational explanations for over 50 operations. Perfect web-based educational tool for students and self-learners.",
    "author": {
    "@type": "Person",
    "name": "Travis Reinart",
    "url": "https://www.linkedin.com/in/travis-reinart/",
    "sameAs": "https://www.linkedin.com/in/travis-reinart/"
    }
}
</script>

    <html lang="en"></html>
    <head>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Exo+2:wght@700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>


    <style>
        :root { --red-accent: #dc2626; --blue-accent: #2563EB; }
        body { 
            font-family: 'Inter', sans-serif; color: #E5E7EB;
            background-image: url('https://raw.githubusercontent.com/treinart/Linear-Algebra-Matrix-Calculator/main/AI_Matrix_Background.jpg');
            background-size: 100% auto;
            background-repeat: no-repeat; background-attachment: fixed; background-position: center;
        }
        .main-container { background-color: rgba(0, 0, 0, 0.75); backdrop-filter: blur(8px); }
        .main-title { 
            font-family: 'Exo 2', sans-serif; 
            background: linear-gradient(45deg, #3B82F6, #A855F7, #EC4899);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .tab-button {
            padding: 0.625rem 1.25rem; margin: 0.25rem; font-size: 1rem; font-weight: 600;
            color: white; background-color: var(--blue-accent); border: 2px solid var(--blue-accent);
            border-radius: 0.5rem; transition: all 0.2s ease-in-out; cursor: pointer;
        }
        .tab-button.active, .tab-button:hover { background-color: var(--red-accent); border-color: var(--red-accent); }
        .matrix-input, .scalar-input {
            width: 5rem; height: 3rem; font-size: 1.1rem;
            text-align: center; background-color: #374151;
            border: 1px solid #4B5563; border-radius: 0.25rem; color: white; font-weight: 500;
        }
        .control-panel { 
            background-color: rgba(30, 41, 59, 0.5); 
            border: 1px solid #374151;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 0.75rem;
            padding: 1rem;
        }
        .control-label {
            font-family: 'Exo 2', sans-serif;
            font-size: 1.5rem;
            font-weight: 700;
            color: #d1d5db;
        }
        .styled-select {
            background-color: #1F2937; border: 1px solid #4B5563; color: #E5E7EB; padding: 0.5rem 2.5rem 0.5rem 0.75rem;
            border-radius: 0.375rem; -webkit-appearance: none; -moz-appearance: none; appearance: none;
            background-image: url("data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' fill='none' viewBox='0 0 20 20'%3e%3cpath stroke='%239CA3AF' stroke-linecap='round' stroke-linejoin='round' stroke-width='1.5' d='M6 8l4 4 4-4'/%3e%3c/svg%3e");
            background-position: right 0.5rem center; background-repeat: no-repeat; background-size: 1.5em 1.5em;
        }
        .explainer-container h3 { 
            font-size: 1.75rem; font-weight: 700; margin-top: 2.5rem; margin-bottom: 1rem; padding-bottom: 0.5rem;
            background: linear-gradient(to right, var(--blue-accent), var(--red-accent));
            background-repeat: no-repeat; background-size: 100% 2px; background-position: 0 100%;
        }
        .explainer-container h3:first-child { margin-top: 0; }
        .explainer-text { font-size: 1.1rem; color: #D1D5DB; margin-bottom: 1rem; line-height: 1.6; }
        .formula-box { font-size: 1.25rem; font-family: 'Courier New', monospace; text-align: center; margin: 1.5rem 0; padding: 1rem; background-color: rgba(0,0,0,0.3); border-radius: 0.5rem;}
        .matrix-display-container { display: flex; justify-content: center; align-items: center; gap: 2rem; flex-wrap: wrap; margin: 2rem 0; }
        .matrix-bracket { position: relative; display: inline-block; padding: 0 10px; }
        .matrix-bracket::before, .matrix-bracket::after {
            content: ''; position: absolute; top: -10%; bottom: -10%; width: 12px;
            border-style: solid; border-color: var(--blue-accent);
        }
        .matrix-bracket::before { left: -12px; border-width: 3px 0 3px 3px; border-radius: 10px 0 0 10px; }
        .matrix-bracket::after { right: -12px; border-width: 3px 3px 3px 0; border-radius: 0 10px 10px 0; border-color: var(--red-accent); }
        .matrix-content { padding: 0.5rem; }
        .matrix-content table { border-collapse: collapse; }
        .matrix-content td { padding: 0.75rem; text-align: center; font-family: 'Courier New', monospace; font-size: 1.5rem; }
        .operator { font-size: 2rem; color: #9CA3AF; }
        #plot-container { max-width: 600px; margin: auto; background-color: rgba(255,255,255,0.9); border-radius: 0.5rem; padding: 1rem; }
        
        .overview-details {
            margin-bottom: 1.5rem;
            border: 1px solid #374151;
            border-radius: 0.5rem;
            background-color: rgba(30, 41, 59, 0.3);
        }
        .overview-summary {
            font-size: 1.25rem;
            font-weight: 600;
            padding: 1rem;
            cursor: pointer;
            list-style: none; /* Hide default marker */
        }
        .overview-summary::-webkit-details-marker { display: none; } /* Hide default marker */
        .overview-summary::before {
            content: '[+]';
            margin-right: 0.75rem;
            font-family: 'Courier New', monospace;
        }
        .overview-details[open] > .overview-summary::before {
            content: '[-]';
        }
        .overview-content {
            padding: 0 1.5rem 1.5rem 1.5rem;
            border-top: 1px solid #374151;
        }
        .overview-content p {
            margin-bottom: 1rem;
            font-size: 1.1rem;
            color: #D1D5DB;
            line-height: 1.6;
        }
    </style>
    </head>
    <body class="p-4 md:p-8">

    <script>
    console.log(
    "\n\n\n" +
    "%c                TRAVIS IS #1\n" +
    "%c              TRAVIS IS #1\n" +
    "%c            TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c                TRAVIS IS #1\n" +
    "%c   TRAVIS IS #1 TRAVIS IS #1 TRAVIS IS #1\n\n\n",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;",
    "color: green; font-weight: bold; font-size: 18px;"
    );

    const year = new Date().getFullYear();
    console.log(
    `%c¬© ${year} Travis Reinart - Linear Algebra Matrix Calculator`,
    "color: gold; font-size: 16px; font-weight: bold;"
    );
    console.log(
    "%cLicensed under CC BY-ND 4.0",
    "color: gold; font-size: 13px; font-weight: normal;"
    );
    console.log(
    "%chttps://creativecommons.org/licenses/by-nd/4.0/",
    "color: gold; font-size: 13px; font-weight: normal; text-decoration: underline;"
    );
    console.log(
    "\n%cüîóüü¶ Connect on LinkedIn: %chttps://www.linkedin.com/in/travis-reinart/\n",
    "color: crimson; font-size: 15px; font-weight: bold;",
    "color: black; font-size: 15px; text-decoration: underline;"
    );
    </script>

    <header class="container mx-auto px-4 sm:px-6 lg:px-8 py-4 flex items-center justify-between main-container rounded-b-xl shadow-lg">
        <div>
            <h1 class="text-4xl font-bold main-title">Linear Algebra Matrix Calculator</h1>
            <p class="text-lg text-gray-300 font-bold">Your tool for quick matrix operations and learning</p>
        </div>
        <div class="text-right">
            <h2 class="text-xl font-bold" style="color: #ff0202;">TRAVIS IS #1</h2>
        </div>
    </header>

    <main class="container mx-auto p-4 md:p-8 max-w-7xl mt-6 main-container rounded-xl shadow-lg">
        <div class="mb-6">
            <nav id="operation-tabs" class="flex flex-wrap justify-center gap-2"></nav>
        </div>
        <div id="calculator-body"></div>
    </main>

    <footer class="text-center py-6 text-sm text-white/80 mt-8">
    <p class="font-bold" style="color: green;">
        &copy; <span id="currentYear"></span> Travis Reinart - Linear Algebra Matrix Calculator
    </p>
    <p>
        <a href="https://creativecommons.org/licenses/by-nd/4.0/" target="_blank" rel="noopener" style="color: gold; font-weight: bold;">
            CC BY-ND 4.0 License
        </a>
    </p>
    <p>
        <a href="https://www.linkedin.com/in/travis-reinart/" target="_blank" style="color: crimson; font-weight: bold;">Connect on LinkedIn</a>
    </p>
    </footer>

    <script>
    (function() {
    let chartInstance = null;
    const operationsData = {
        "matrix-add-subtract": { 
            id: "matrix-add-subtract", name: "Matrix & Vector Addition/Subtraction", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }, { type: 'matrix', name: 'Matrix B' }], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'} ],
            overview: `Matrix and vector addition or subtraction lets you combine or separate objects that have the same size and shape. When you add two matrices, you add their entries one by one. The same process applies to vectors. If you subtract, you do the same thing but take away the second entry from the first at each position.\nThis operation is everywhere in applied math and science. For example, engineers use matrix addition to track multiple physical systems at once, and economists might combine data tables for different time periods. You have to make sure the objects are the same size, or the math won‚Äôt work. It is a basic building block for modeling, simulations, and more complex operations later on.\nThe rules for addition and subtraction in linear algebra date back to the origins of matrices in the 1800s. While the idea is simple, it lays the groundwork for much deeper topics like solving systems, analyzing signals, and transforming data.`
        },
        "scalar-mult-matrix": { 
            id: "scalar-mult-matrix", name: "Scalar Multiplication (Matrix)", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }, { type: 'scalar', name: 'Scalar' }], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'} ],
            overview: `Scalar multiplication is a way to stretch or shrink a matrix without changing its shape. You take a single number, called a scalar, and multiply it by every entry in the matrix. The process leaves the structure intact but changes the magnitude of each entry.\nIn real life, scalar multiplication lets you rescale datasets, adjust weights in models, or apply gain to a system in engineering. If you multiply by a positive number, everything gets bigger or smaller in the same direction. If you multiply by a negative number, you also flip the direction of all effects.\nThe operation is a foundation for matrix algebra and makes it easy to combine, compare, and manipulate large collections of numbers. Its simplicity keeps it relevant from the earliest days of linear algebra all the way to modern data analysis and machine learning.`
        },
        "transpose": { 
            id: "transpose", name: "Transpose", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x3'}, {text: '3x4'} ],
            overview: `Transposing a matrix means flipping it over its diagonal, swapping rows and columns. If you have a matrix with three rows and two columns, the transpose will have two rows and three columns. This simple idea turns out to be powerful in linear algebra and beyond.\nPeople use the transpose when working with dot products, solving certain systems of equations, or switching the focus between variables and equations in a dataset. In statistics, transposing lets you work with different perspectives on the same data. In computer graphics, you might need to transpose to change between row-major and column-major storage.\nThe concept has roots in the early study of matrices, when mathematicians started looking for systematic ways to rearrange and analyze large tables of numbers. Today, the transpose is a core part of almost every branch of applied math.`
        },
        "symmetric-matrix": { 
            id: "symmetric-matrix", name: "Symmetric Matrix", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'} ],
            overview: `A symmetric matrix is a square matrix that looks the same above and below its diagonal. In other words, the entry at position (i, j) always matches the entry at (j, i). Symmetric matrices show up naturally in physics, engineering, and statistics whenever relationships are mutual‚Äîlike how the force between two objects is the same in both directions.\nThese matrices have some special properties that make them easier to work with. For example, their eigenvalues are always real numbers, and you can always diagonalize them with the right rotation. In statistics, covariance matrices are symmetric because the correlation between variable A and B is the same as between B and A.\nSymmetric matrices have been studied since the 1800s, and they remain central to everything from vibration analysis in engineering to principal component analysis in data science. Their balance and structure make them both practical and mathematically elegant.`
        },
        "matrix-multiplication": { 
            id: "matrix-multiplication", name: "Matrix Multiplication", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }, { type: 'matrix', name: 'Matrix B' }], 
            sizes: [ {text: '2x2 * 2x2'}, {text: '2x3 * 3x2'}, {text: '3x2 * 2x3'}, {text: '3x3 * 3x3'} ],
            overview: `Matrix multiplication links two matrices in a way that blends the rows of the first matrix with the columns of the second. This operation is not as simple as adding or multiplying entries one by one. Instead, each entry in the product matrix is the sum of products from a row and a column, like matching questions with answers.\nThis method turns out to be essential in describing linear transformations, combining systems, or chaining together operations in physics and engineering. In computer graphics, matrix multiplication moves and rotates objects on the screen. In economics, it can model flows between sectors of an economy.\nThe rules for matrix multiplication were set in the mid-1800s, with contributions from mathematicians like Arthur Cayley. This operation forms the backbone of linear algebra and appears in almost every technical field that uses mathematics.`
        },
        "augment-matrix": { 
            id: "augment-matrix", name: "Augment Matrix", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }, { type: 'vector', name: 'Vector b' }], 
            sizes: [ { text: '2x2 & 2D'}, { text: '3x3 & 3D' } ],
            overview: `An augmented matrix is a tool for solving systems of equations more efficiently. You build it by writing the coefficient matrix and then tacking on the right-hand side of the equations as an extra column. This format makes it easy to use row operations, which are steps that help you solve for unknowns without juggling lots of equations at once.\nThe augmented matrix method is especially useful in Gaussian elimination, which is a step-by-step way to simplify and solve systems. You see augmented matrices in basic algebra classes, engineering software, and research papers. They keep your work organized and help automate the process.\nThis approach gained popularity in the late 1800s and early 1900s as mathematicians and scientists began solving larger and more complicated systems. Today, the augmented matrix is a go-to format for many computational algorithms.`
        },
        "vector-add-subtract": { 
            id: "vector-add-subtract", name: "Vector Addition/Subtraction", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }, { type: 'vector', name: 'Vector v' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `Vector addition and subtraction are the bread and butter of working with direction and movement. To add two vectors, you line up their entries and add each pair. Subtracting works the same way, but you take away instead of combine. The result is a new vector that represents the combined movement or the difference in direction and size.\nYou see these operations in almost every field that deals with forces, movement, or change. Physicists use them to find the net force on an object. Engineers add vectors to describe displacements or velocities. In economics, vector addition can show the effect of multiple factors on a single outcome.\nThe idea goes back to the earliest days of geometry and mechanics. The rules remain simple, but they build the foundation for more advanced ideas in linear algebra, physics, and engineering.`
        },
        "scalar-mult-vector": { 
            id: "scalar-mult-vector", name: "Scalar Multiplication (Vector)", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }, { type: 'scalar', name: 'Scalar' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `Scalar multiplication takes a vector and stretches or shrinks it by multiplying every entry by a single number. That number is the scalar. The operation changes the length of the vector but keeps its direction, unless you multiply by a negative, which flips the direction.\nYou use scalar multiplication to scale forces in physics, resize quantities in economics, or create proportional changes in engineering models. In graphics, it‚Äôs how you stretch or shrink shapes along a line.\nScalar multiplication is one of the simplest but most important tools in linear algebra. It makes it possible to combine vectors and build new spaces, and it‚Äôs the first step in forming linear combinations.`
        },
        "dot-product": { 
            id: "dot-product", name: "Dot Product", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }, { type: 'vector', name: 'Vector v' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' }, { text: '5D' }, { text: '6D' } ],
            overview: `The dot product is a way to combine two vectors and get a single number out. You multiply each pair of corresponding entries and add them up. This number tells you how much the vectors point in the same direction. If the result is large and positive, they are aligned. If it is zero, the vectors are perpendicular. If it is negative, they point away from each other.\nPeople use the dot product to measure similarity in machine learning, project forces in physics, and even compute lighting in computer graphics. The operation is also key to finding angles between vectors and checking orthogonality.\nMathematicians have used the dot product since the 1800s to simplify work with vectors. The concept has become central in geometry, engineering, and data science.`
        },
        "cross-product": { 
            id: "cross-product", name: "Cross Product", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }, { type: 'vector', name: 'Vector v' }], 
            sizes: [ { text: '3D' } ],
            overview: `The cross product creates a new vector that is perpendicular to two given vectors in three-dimensional space. Its length shows the area of the parallelogram spanned by the two input vectors. The cross product has a direction given by the right-hand rule.\nYou will find cross products in physics whenever you need to calculate torque or rotational force. Engineers use them to find the normal vector to a surface, which is important for stability and design. In computer graphics, cross products help determine how light hits a surface.\nThe cross product came into mathematics through work by William Rowan Hamilton and others in the 19th century. It remains an essential tool wherever you work with vectors in three dimensions.`
        },
        "vector-length-norm": { 
            id: "vector-length-norm", name: "Vector Length (Norm)", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `The norm, or length, of a vector measures how far it reaches from the origin. To find it, you square each entry, add them up, and then take the square root of the sum. The result is always non-negative and tells you the size or magnitude of the vector.\nNorms are everywhere in science and engineering. They let you measure distances, calculate errors, and set thresholds in optimization. In geometry, the norm gives you the ‚Äústraight-line distance‚Äù between points.\nThe concept of vector length dates back to the Pythagorean theorem and forms a cornerstone of both old and modern mathematics. Norms make it possible to talk about size and closeness in high-dimensional spaces.`
        },
        "unit-vector": { 
            id: "unit-vector", name: "Unit Vector", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `A unit vector is a vector with length one that points in a specific direction. You create a unit vector by dividing any nonzero vector by its own norm. The result keeps the direction but resets the magnitude to one.\nUnit vectors are the language of direction in math, science, and engineering. You use them to break down forces into components, define directions in space, and build orthonormal bases in more advanced applications. In computer graphics, unit vectors tell you where things point.\nThe idea is simple but powerful, giving you a standard way to represent any direction, no matter how complicated the system.`
        },
        "angle-between-vectors": { 
            id: "angle-between-vectors", name: "Angle Between Vectors", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }, { type: 'vector', name: 'Vector v' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `The angle between two vectors gives you a way to measure how much they point in the same or different directions. You can calculate this angle using the dot product, which links the vectors' lengths and how much they overlap. When two vectors line up exactly, their angle is zero degrees; if they point in opposite directions, the angle is 180 degrees. When the vectors are at right angles, the dot product drops to zero, showing perfect independence.\nThis idea turns up in dozens of settings. In physics, you use it to figure out the component of one force acting along another direction. In data science, it helps measure similarity between data points‚Äîwhen two vectors are close together, their angle is small, showing they represent similar things. In computer vision and graphics, angle calculations tell you how surfaces catch light or how to rotate shapes.\nThe link between vector angles and the dot product is rooted in geometry and trigonometry, but it has become an everyday tool in modern fields like machine learning and robotics. The concept stays relevant because it offers a direct bridge between geometry and algebra.`
        },
        "vector-ops": { 
            id: "vector-ops", name: "Vector Ops", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }, { type: 'vector', name: 'Vector v' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `Vector operations bring together a range of tools you use to analyze direction, size, and change. These include adding and subtracting vectors, scaling them, taking dot and cross products, and even projecting one vector onto another. Together, these basic operations help you answer big questions: Where will something end up if you move it in several steps? How closely do two signals or directions match? What happens if you apply force in a particular way?\nIn physics and engineering, vector operations form the grammar for describing movement, force, and energy. A pilot uses vector addition to chart a course against the wind. An engineer calculates the dot product to see how much of a force moves an object forward. Cross products show up in anything with rotation or 3D geometry, such as drone movement or mechanical linkages.\nThese building blocks have roots going back to the 19th century, when William Rowan Hamilton and others formalized vector algebra. Today, vector operations shape everything from video game design to climate modeling.`
        },
        "2d-vector-plotter": { 
            id: "2d-vector-plotter", name: "2D Vector Plotter", implemented: true, 
            inputs: [{ type: 'multivector', name: 'Vectors to Plot', dim: 2 }], 
            sizes: [ {text: '1 Vector'}, {text: '2 Vectors'}, {text: '3 Vectors'}, {text: '4 Vectors'} ],
            overview: `A 2D vector plotter takes numerical data and shows it as an arrow on a flat plane. It helps you see both the direction and length of the vector in a way that's much easier to grasp than looking at numbers alone. By plotting multiple vectors, you can visualize how they add together, see patterns, or explore the effects of different operations.\nTeachers and students use 2D vector plotting to build intuition for vector addition and subtraction. In navigation, you might plot wind and current vectors to see how a boat or plane will move. Designers and artists use similar plots to explore forces or alignments in digital tools.\nVector visualization goes back to early engineering drawings but became much more practical with computers. Today, plotting vectors is standard practice in education, science, and engineering.`
        },
        "linear-combination": { 
            id: "linear-combination", name: "Linear Combination", implemented: true, 
            inputs: [{ type: 'multivector_scalar', name: 'Vectors and Scalars', dim: 'any' }], 
            sizes: [ {text: '2 Vectors'}, {text: '3 Vectors'}, {text: '4 Vectors'} ], 
            dimSizes: [ {text: '2D'}, {text: '3D'}, {text: '4D'} ],
            overview: `A linear combination is a way to build new vectors by scaling some set of original vectors and adding the results. This idea lies at the heart of linear algebra. If you can form a vector as a linear combination of others, it means you can reach that point by combining the originals with the right scaling factors.\nYou see linear combinations everywhere. In engineering, signals often get mixed and reshaped as combinations of basic waveforms. In economics, portfolios are built as linear combinations of investments. In data science, features are combined to create new variables that capture key patterns.\nThe concept is old, but it remains central because it helps answer a fundamental question: Can I get from here to there with the pieces I have? Understanding linear combinations opens the door to span, independence, basis, and dimension.`
        },
        "span": { 
            id: "span", name: "Span", implemented: true, 
            inputs: [{ type: 'multivector_target', name: 'Vector Set', dim: 'any' }], 
            sizes: [ {text: '2 Vectors'}, {text: '3 Vectors'}, {text: '4 Vectors'} ], 
            dimSizes: [ {text: '2D'}, {text: '3D'}, {text: '4D'} ],
            overview: `The span of a set of vectors is the collection of all possible places you can reach by taking linear combinations of them. If you think of each vector as a move, the span is all the destinations you could reach by following any sequence of moves with any amount of each one.\nSpan is more than an abstract idea. In robotics, it tells you what spaces a robot arm can reach. In statistics, the span of predictor variables shows which outcomes your model can describe. In graphics, the span of colors from a set of inks or lights determines what shades you can produce.\nThe concept of span helps define the power and limits of any system. If your vectors span the whole space, you have the freedom to reach any point. If not, you‚Äôre locked into a smaller world.`
        },
        "linear-independence": { 
            id: "linear-independence", name: "Linear Independence", implemented: true, 
            inputs: [{ type: 'multivector', name: 'Set of Vectors', dim: 'any' }], 
            sizes: [ {text: '2 Vectors'}, {text: '3 Vectors'}, {text: '4 Vectors'} ], 
            dimSizes: [ {text: '2D'}, {text: '3D'}, {text: '4D'} ],
            overview: `Linear independence tests whether any vector in a set can be built from the others using linear combinations. If you cannot do this, the set is called independent. If you can, the set is dependent. Independence means each vector brings something new to the table.\nThis test has big implications. In engineering, independent directions let you control a machine fully. In statistics, independent variables ensure you can separate the effect of each factor. In coding and data storage, independence means you avoid redundancy.\nThe idea became formal in the 1800s as mathematicians built the first theories of vector spaces. Today, independence is a tool for simplifying models, designing robust systems, and understanding the true ‚Äúsize‚Äù or capability of a set.`
        },
        "basis-dimension": { 
            id: "basis-dimension", name: "Basis & Dimension", implemented: true, 
            inputs: [{ type: 'multivector', name: 'Set of Vectors' }], 
            sizes: [ {text: '2D'}, {text: '3D'}, {text: '4D'} ],
            overview: `A basis is a set of vectors that, when combined, can reach any point in a space without any redundancy. Each vector in the basis points in a unique direction, and together, they give you the building blocks for the entire space. The number of vectors in the basis is called the dimension, and it tells you how many independent directions you have to move.\nYou see bases at work when you describe a location with coordinates. The classic x, y, and z axes in three dimensions are a basis for 3D space. In data science, basis vectors can represent independent features that build all possible data points. Choosing a good basis can simplify complex problems, reduce computation, and help you find patterns.\nThe idea of basis and dimension is central to both pure and applied mathematics. It gives you a way to count, organize, and make sense of complex systems, from engineering designs to large data sets.`
        },
        "orthogonal-vectors": { 
            id: "orthogonal-vectors", name: "Orthogonal Vectors", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u' }, { type: 'vector', name: 'Vector v' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `Two vectors are orthogonal if they meet at a right angle, or in mathematical terms, their dot product is zero. This relationship signals perfect independence: one vector has no component in the direction of the other. In three or more dimensions, sets of orthogonal vectors give you a way to break up space into clean, non-overlapping directions.\nOrthogonal vectors play a starring role in physics, where forces, fields, or signals often need to be split into components. In data science and statistics, orthogonal features mean no overlap in information, which helps make models clearer and more interpretable. In graphics and engineering, orthogonal vectors define axes and help keep systems stable.\nOrthogonality is a simple test for independence and clarity in math and science. It helps you cleanly separate parts of a problem and build systems that are easier to manage and understand.`
        },
        "orthogonal-projection": { 
            id: "orthogonal-projection", name: "Orthogonal Projection", implemented: true, 
            inputs: [{ type: 'vector', name: 'Vector u (to project)' }, { type: 'vector', name: 'Vector v (project onto)' }], 
            sizes: [ { text: '2D' }, { text: '3D' }, { text: '4D' } ],
            overview: `Orthogonal projection lets you ‚Äúdrop‚Äù one vector onto another or onto a subspace, finding the closest point to the original within the span of the target. Think of shining a flashlight straight down: the shadow shows the orthogonal projection. This operation gives you the part of a vector that lines up with a chosen direction or set of directions, and the leftover is perpendicular.\nIn real life, orthogonal projections help engineers find components of forces, designers minimize errors, and data scientists fit models to data. They are also at the heart of least squares regression, where you project data onto the space described by your predictors to find the best fit.\nThe process traces back to basic geometry, but in modern mathematics, projections have become essential in statistics, optimization, and numerical methods. They make complex relationships manageable by breaking them into parts you can analyze and control.`
        },
        "orthogonal-complement": { 
            id: "orthogonal-complement", name: "Orthogonal Complement", implemented: true, 
            inputs: [{ type: 'multivector', name: 'Set of Vectors', dim: 'any' }], 
            sizes: [ {text: '1 Vector'}, {text: '2 Vectors'}, {text: '3 Vectors'} ], 
            dimSizes: [ {text: '2D'}, {text: '3D'}, {text: '4D'} ],
            overview: `The orthogonal complement of a subspace is the set of all vectors that are perpendicular to every vector in the subspace. If your subspace is a plane in three dimensions, the orthogonal complement is the line passing through the origin that is perpendicular to that plane. Together, a space and its orthogonal complement fill out the whole environment without any overlap.\nOrthogonal complements show up in engineering when separating movements into independent parts, such as up-down and side-to-side in vibrations. In statistics, they help explain variance not accounted for by a model. In control systems, the concept helps design feedback that only responds to errors, not signals you want to keep.\nThis idea is as much about what you cannot do as what you can. It gives you a language for constraints, error correction, and for understanding the hidden structure in data and systems.`
        },
        "gram-schmidt": { 
            id: "gram-schmidt", name: "Gram-Schmidt Process", implemented: true, 
            inputs: [{ type: 'multivector', name: 'Set of Vectors', dim: 'any' }], 
            sizes: [ {text: '2 Vectors'}, {text: '3 Vectors'}, {text: '4 Vectors'} ], 
            dimSizes: [ {text: '2D'}, {text: '3D'}, {text: '4D'} ],
            overview: `The Gram-Schmidt process is a method for taking a messy set of vectors and turning them into a neat set of orthogonal (and often orthonormal) vectors that point in the same directions. By systematically removing overlap, Gram-Schmidt helps you build a clean basis for a space where each direction stands alone.\nYou use Gram-Schmidt in everything from QR decomposition to constructing stable coordinate systems in engineering. In statistics and machine learning, it helps create independent features or factors. The process is also a backbone for numerical algorithms, where stable, independent directions reduce errors.\nThis method was named after J√∏rgen Gram and Erhard Schmidt, who worked on orthogonalization in the early 1900s. Today, it is a key tool for organizing information, simplifying problems, and building strong mathematical foundations.`
        },
        "ref": { 
            id: "ref", name: "Row Echelon Form (REF)", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '3x4'}, {text: '4x3'}, {text: '4x4'} ],
            overview: `Row Echelon Form is a way to tidy up a matrix by using row operations to create zeros below each leading entry (pivot) as you move from left to right. Each pivot appears to the right of the pivot above it, and rows of zeros, if any, are at the bottom. The result is a ‚Äústaircase‚Äù shape that makes it much easier to solve systems or understand the structure of a matrix.\nREF is a workhorse in solving systems of linear equations. It forms the basis of Gaussian elimination, which is how many calculators and computer programs find solutions quickly. You also use it to find rank and spot dependencies between equations.\nMathematicians and engineers have relied on this technique for centuries because it brings order and simplicity to complex problems. With REF, you can see at a glance how many solutions a system has and how to find them.`
        },
        "step-by-step-rref": { 
            id: "step-by-step-rref", name: "Step-by-Step RREF", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '3x4'} ],
            overview: `Reduced Row Echelon Form, or RREF, takes the cleanup started by Row Echelon Form and finishes the job. In RREF, each pivot is one, every pivot is the only nonzero entry in its column, and zeros fill out the rest. This form gives you a unique and direct way to read off the solutions to a system or understand the dependencies between variables.\nRREF is central in linear algebra for solving systems, especially when you want to find all solutions, including those with free variables. Computer algorithms rely on RREF to automate calculations, check consistency, and simplify everything from data modeling to cryptography. You can use RREF to determine if a matrix is invertible, spot redundancies, or find the null space directly.\nThis method builds on the idea of row operations, a process formalized in the 1800s and refined as mathematicians and engineers developed ways to solve large, complex systems by hand and then by machine.`
        },
        "determinant-rank": { 
            id: "determinant-rank", name: "Determinant & Rank", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'} ],
            overview: `The determinant is a single number that captures important properties of a square matrix. It measures how much a matrix stretches or shrinks space, and it tells you whether the matrix has an inverse. A zero determinant means the matrix squashes everything into a lower dimension, so no unique inverse exists.\nRank, on the other hand, counts the maximum number of independent rows or columns in a matrix. It tells you how much information is in the matrix or how many directions it can cover. If the rank equals the number of rows or columns, the matrix is full rank and has maximum ‚Äúreach.‚Äù\nBoth determinant and rank show up in every application of linear algebra. Engineers use determinants to check stability in structures. Economists rely on rank to test if their models have enough independent factors. In computer science, both help solve puzzles from network connectivity to coding theory.`
        },
        "identity-inverse": { 
            id: "identity-inverse", name: "Identity & Inverse", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }], 
            sizes: [ { text: '2x2' }, { text: '3x3' }, { text: '4x4' }, { text: '5x5' }, { text: '6x6' } ],
            overview: `The identity matrix is like the number one for matrices. When you multiply any matrix by the identity, the matrix does not change. The identity matrix has ones on the diagonal and zeros everywhere else, making it easy to spot and use.\nThe inverse of a matrix, if it exists, is another matrix that ‚Äúundoes‚Äù the action of the original. When you multiply a matrix by its inverse, you get the identity. Inverses are essential for solving systems of equations, reversing transformations, and finding solutions that work in both directions.\nNot every matrix has an inverse. A matrix must be square and have a nonzero determinant. Finding the inverse is a cornerstone in linear algebra, and it connects to dozens of practical problems in science, business, and engineering.`
        },
        "solve-axb": { 
            id: "solve-axb", name: "Solve Ax = b", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}, { type: 'vector', name: 'Vector b'}], 
            sizes: [ {text: '2x2 & 2D'}, {text: '3x3 & 3D'}, {text: '4x4 & 4D'}, {text: '5x5 & 5D'}, {text: '6x6 & 6D'} ],
            overview: `Solving Ax=b is the heart of linear algebra. Here, A is a matrix representing the system, x is a vector of unknowns, and b is the outcome you want. To find x, you use methods like elimination, substitution, or matrix inversion. The answer tells you how to combine the columns of A to get to b.\nThis approach is everywhere in science and industry. Engineers use it to design structures that stand up to loads. Economists use it to balance supply and demand. Computer scientists use it to solve networks, graphics, and even search algorithms.\nThe process has evolved from manual calculation with pencil and paper to massive systems solved by computers. Understanding how to set up and solve Ax=b opens doors in any field that uses quantitative models.`
        },
        "least-squares": { 
            id: "least-squares", name: "Least Squares", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}, { type: 'vector', name: 'Vector b'}], 
            sizes: [ {text: '2x2 & 2D'}, {text: '3x2 & 3D'}, {text: '3x3 & 3D'}, {text: '4x2 & 4D'}, {text: '4x3 & 4D'}, {text: '4x4 & 4D'}, {text: '5x3 & 5D'}, {text: '5x4 & 5D'}, {text: '5x5 & 5D'}, {text: '6x4 & 6D'}, {text: '6x5 & 6D'}, {text: '6x6 & 6D'} ],
            overview: `Least squares is a method for finding the best-fit solution to a system of equations that might not have an exact answer. It works by minimizing the sum of the squares of the differences (errors) between observed values and those predicted by your model. In practice, this means you find the closest possible solution in a messy, real-world setting.\nYou see least squares everywhere data does not line up perfectly, such as in regression analysis, curve fitting, or signal processing. Statisticians, engineers, and scientists use it to handle noise, outliers, and uncertainty.\nThe least squares method dates back to Carl Friedrich Gauss and Adrien-Marie Legendre in the early 1800s. It remains a gold standard for estimation and model fitting because it is mathematically solid and practical for large datasets.`
        },
        "eigenvalues-eigenvectors": { 
            id: "eigenvalues-eigenvectors", name: "Eigenvalues & Eigenvectors", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Eigenvalues and eigenvectors reveal the hidden structure in a matrix. An eigenvector is a direction that a matrix stretches or shrinks but does not rotate. The eigenvalue tells you how much the stretch or shrink happens along that direction. Together, they explain how systems evolve, resonate, or change over time.\nIn engineering, eigenvalues predict natural frequencies and vibrations. In data science, they drive principal component analysis, reducing dimensions while keeping the core patterns. In physics, they describe stable states and transitions in quantum mechanics.\nThe study of eigenvalues and eigenvectors began in the 19th century with work by Augustin-Louis Cauchy and others. These concepts have become some of the most powerful tools for understanding systems, simplifying complex processes, and unlocking deeper insights across science and engineering.`
        },
        "characteristic-equation": { 
            id: "characteristic-equation", name: "Characteristic Equation", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The characteristic equation is a polynomial equation that you build from a square matrix by subtracting a variable times the identity matrix from your original matrix and setting the determinant equal to zero. The solutions to this equation, called the characteristic roots, are the eigenvalues of the matrix. Finding these eigenvalues reveals the scaling factors for special directions where the matrix‚Äôs action is simple.\nEngineers use the characteristic equation to predict the stability of mechanical and electrical systems. In physics, it helps analyze vibrations and quantum states. In finance, it helps model risk in large portfolios. Any time you want to understand the deep structure of a transformation or how a system behaves over time, you‚Äôll find the characteristic equation in the background.\nThe method dates back to the 19th century and links algebra to geometry by showing how the complex behavior of a system can be distilled into roots of a simple polynomial. It is a core step in eigenvalue analysis and advanced linear algebra applications.`
        },
        "diagonalization": { 
            id: "diagonalization", name: "Diagonalization", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Diagonalization is a technique that transforms a square matrix into a diagonal form using a change of basis, provided the matrix meets certain conditions. In the diagonalized form, all the action happens along the main diagonal, making calculations much easier. The entries on the diagonal are the eigenvalues, and the new basis vectors are the eigenvectors.\nThis process is invaluable in solving systems of differential equations, computing matrix powers, and simplifying dynamic systems. When you diagonalize, you make it much easier to understand repeated actions, such as applying the same transformation multiple times or modeling growth and decay.\nDiagonalization has its roots in 19th-century mathematics and still sits at the heart of quantum mechanics, vibration analysis, and statistics. Not every matrix can be diagonalized, but when it can, you unlock a direct and transparent view into how a system works.`
        },
        "svd": { 
            id: "svd", name: "Singular Value Decomposition (SVD)", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '3x4'}, {text: '4x3'}, {text: '4x4'}, {text: '5x4'}, {text: '4x5'}, {text: '5x5'}, {text: '5x6'}, {text: '6x5'}, {text: '6x6'} ],
            overview: `Singular Value Decomposition breaks any matrix‚Äîsquare or rectangular‚Äîinto three special matrices: one that rotates, one that stretches, and another that rotates again. The middle matrix contains the singular values, which tell you how much each direction is scaled. SVD is powerful because it works on all matrices, not just nice ones, and reveals hidden structure.\nSVD is everywhere in data science and engineering. You use it in image compression, noise reduction, and solving ill-conditioned systems. It‚Äôs the engine behind principal component analysis, which finds key patterns in big datasets. It also helps design stable systems in control theory.\nThis method was developed through the 20th century and has become a cornerstone for numerical computing, signal processing, and machine learning. SVD‚Äôs ability to break down and rebuild information makes it one of the most important tools in applied mathematics.`
        },
        "hessian-matrix": {
            id: "hessian-matrix", name: "Hessian Matrix", implemented: true,
            inputs: [{ type: 'function', name: 'Function f(x,y) or f(x,y,z)'}],
            sizes: [ {text: 'f(x, y)'}, {text: 'f(x, y, z)'} ],
            overview: `A Hessian matrix is a square matrix built from all the second-order partial derivatives of a function with two or more variables. If you‚Äôre working with a function like f(x, y) or f(x, y, z), the Hessian helps you understand how the function‚Äôs slope changes as you move in any direction. Each entry in the Hessian shows how one variable‚Äôs slope changes as another variable changes, so it captures the full ‚Äúcurvature‚Äù picture.\nYou use the Hessian in calculus, optimization, and machine learning to figure out whether a point is a maximum, minimum, or a saddle point. For example, in optimization problems, checking the Hessian helps you see if you‚Äôve found a true minimum or just a flat spot. The Hessian is also used in physics, engineering, and statistics whenever you want to analyze how something bends or curves.\nThe idea comes from 19th-century mathematics, when researchers needed a tool to generalize the second derivative for functions with more than one variable. Today, the Hessian is standard in any subject that deals with curved surfaces or finding the best (or worst) possible outcome.`
        },
        "lu-decomposition": { 
            id: "lu-decomposition", name: "LU Decomposition", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A' }],
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `LU Decomposition splits a square matrix into the product of a lower triangular matrix (L) and an upper triangular matrix (U). This breakdown makes it faster and more reliable to solve systems of equations or invert matrices, because triangular matrices are much easier to handle. Many algorithms use LU Decomposition as a first step for solving problems that involve large or complex systems.\nIn engineering, LU Decomposition speeds up simulations and analysis. In computer science, it forms the backbone for efficient algorithms in numerical libraries. The process is essential when you need to solve many systems that share the same matrix but have different right-hand sides, since you can reuse the decomposition.\nThe technique dates back to the mid-20th century and has grown in importance as computers have made working with large matrices more common. LU Decomposition is now standard in mathematical software and engineering analysis tools.` 
        },
        "qr-decomposition": { 
            id: "qr-decomposition", name: "QR Decomposition", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `QR Decomposition factors a matrix into an orthogonal matrix (Q) and an upper triangular matrix (R). The Q matrix has columns that are at right angles, and the R matrix is easy to solve. This factorization is especially useful in least squares problems, where you want the best solution even when the system doesn‚Äôt have an exact answer.\nYou‚Äôll find QR Decomposition in statistics for linear regression, in engineering for signal processing, and in scientific computing for eigenvalue algorithms. Because Q is orthogonal, this decomposition is numerically stable and reliable, even when the input matrix has challenging properties.\nThis approach rose to prominence in the 20th century as mathematicians and engineers looked for robust ways to solve equations with real-world data. It remains a key step in many modern data analysis and modeling pipelines.` 
        },
        "cholesky-decomposition": { 
            id: "cholesky-decomposition", name: "Cholesky Decomposition", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Cholesky Decomposition is a way to break a symmetric, positive-definite matrix into the product of a lower triangular matrix and its own transpose. This method is faster and uses less memory than other decompositions for certain types of problems. The Cholesky approach is favored in areas where speed and stability matter and where your matrices meet the required conditions.\nCholesky Decomposition is widely used in statistics, especially in simulations and optimization, because covariance matrices are often symmetric and positive-definite. It‚Äôs also a go-to in engineering for structural analysis and in computer science for solving large systems of equations efficiently.\nFirst published by Andr√©-Louis Cholesky in the early 20th century, this decomposition quickly found a place in numerical methods. It‚Äôs now a backbone in algorithms for everything from finance to machine learning.` 
        },
        "jordan-canonical-form": { 
            id: "jordan-canonical-form", name: "Jordan Canonical Form", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The Jordan Canonical Form is a way to rewrite a square matrix so its structure becomes easier to see and understand. You reorganize the matrix into blocks, each related to an eigenvalue. This form reveals how the matrix acts on different directions, even if you cannot diagonalize it in the usual way.\nYou use the Jordan form in advanced areas like control theory, differential equations, and some parts of quantum physics. It helps when a system has repeated eigenvalues or lacks enough independent eigenvectors. By showing each piece of the action, the Jordan form gives you tools to solve equations that involve repeated or tangled behavior.\nCamille Jordan introduced this concept in the late 1800s. While it may sound abstract, the Jordan Canonical Form remains valuable for anyone who needs a full breakdown of a system‚Äôs possible motions or transformations.` 
        },
        "minimal-polynomial": { 
            id: "minimal-polynomial", name: "Minimal Polynomial", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The minimal polynomial of a square matrix is the simplest polynomial you can build, with a leading coefficient of one, that makes the matrix behave like zero when you plug it in. In other words, if you substitute the matrix into this polynomial, you get the zero matrix. The minimal polynomial gives you a shortcut for understanding how a matrix transforms space.\nThis idea helps you decide if a matrix can be diagonalized or if it will always have some ‚Äúrepeated‚Äù action you cannot untangle. Engineers and scientists use the minimal polynomial in system theory and in studying vibrations and dynamic systems. It can tell you if a complicated set of rules can really be boiled down to something simple.\nWork on minimal polynomials became part of mathematics in the late 19th and early 20th centuries, when people wanted to classify the behavior of all possible matrices. It is still used by anyone who needs to peek under the hood of a transformation.` 
        },
        "matrix-exponential": { 
            id: "matrix-exponential", name: "Matrix Exponential / Powers", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The matrix exponential is a tool for describing processes that change continuously, like growth, rotation, or decay. When you raise a matrix to a power or find its exponential, you can model how systems develop over time. The exponential connects discrete steps to smooth changes, letting you solve systems of differential equations or predict what will happen as you apply the same transformation again and again.\nThe matrix logarithm reverses this idea, letting you find the transformation that would create a given change if you repeated it continuously. These ideas show up in physics, engineering, and finance‚Äîwhere you want to model compound interest, population growth, or the spread of energy.\nMatrix exponentials were explored as early as the 19th century but became truly useful with the arrival of computers. They give us a way to step between the world of discrete and continuous mathematics with confidence.` 
        },
        "null-space": { 
            id: "null-space", name: "Null Space / Kernel", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The null space, also called the kernel, is the set of all vectors that a matrix turns into zero when you multiply. In simple terms, it describes the ‚Äúinvisible‚Äù directions that vanish under the matrix‚Äôs rules. If you solve the equation Ax = 0, all the solutions live in the null space.\nYou find the null space in engineering when you look for modes of a structure that do not produce movement or force. In data science, the null space points out hidden relationships between variables. If the null space is just the zero vector, your matrix has full rank and no redundancy. If it is bigger, something in your system overlaps or repeats.\nMathematicians formalized the idea of the null space to classify and understand linear transformations. Today, it remains a key test for independence, redundancy, and the underlying structure of any system.` 
        },
        "column-row-space": { 
            id: "column-row-space", name: "Column Space & Row Space", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The column space of a matrix is the collection of all possible outcomes you can reach by mixing its columns with different weights. The row space is built by looking at all combinations of the rows. These spaces tell you the range and coverage of your matrix‚Äîwhat it can and cannot do.\nIn practice, column and row spaces help you check if a system of equations has a solution and whether your data model has enough reach to cover every outcome you care about. In engineering, these spaces show which signals or states are possible. In statistics and machine learning, column space relates to what your predictors can explain.\nThese ideas have been with linear algebra since its beginnings. They give you a simple way to see the power and limitations of a matrix in any field.` 
        },
        "image-rank-factorization": { 
            id: "image-rank-factorization", name: "Image & Rank Factorization", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The image of a matrix, also called its range, is the set of all results you can get by applying the matrix to some input. Rank factorization breaks a matrix down into two smaller pieces: one with independent columns, and the other with instructions for how to combine them. This helps you see how much unique information your matrix holds and how to rebuild it from a simpler form.\nEngineers and data scientists use image and rank factorization to reduce complexity, compress data, and spot the core drivers in a system. In economics, you might use rank factorization to summarize markets with a few key trends. In computing, these tools help optimize and speed up calculations.\nThese concepts have their roots in 20th-century mathematics, when researchers started to look for efficient ways to store and process big datasets. Image and rank factorization let you focus on what really matters without losing the thread of the bigger picture.` 
        },
        "change-of-basis": { 
            id: "change-of-basis", name: "Change of Basis Matrix", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix P (New to Old)'}, { type: 'vector', name: 'Vector x (Old Basis)'}], 
            sizes: [ {text: '2x2 & 2D'}, {text: '3x3 & 3D'}, {text: '4x4 & 4D'}, {text: '5x5 & 5D'}, {text: '6x6 & 6D'} ],
            overview: `A change of basis matrix helps you translate between different coordinate systems in a vector space. When you switch from one set of basis vectors to another, you use this matrix to convert the description of vectors or transformations. This process makes it possible to simplify problems, see patterns, or make calculations easier by working in the most natural or convenient coordinate system.\nYou use change of basis in engineering when switching from one reference frame to another, such as moving from the axes of a building to the axes of a machine inside it. In data science, you might change the basis to reduce the number of variables or focus on the most important features in your data. In computer graphics, basis changes allow you to rotate or scale objects with ease.\nThe idea traces back to the heart of linear algebra. The power of changing basis is that it gives you control over how you see and work with your data, letting you choose the most useful perspective for the problem at hand.` 
        },
        "laplace-expansion": { 
            id: "laplace-expansion", name: "Determinant by Laplace Expansion", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Laplace expansion gives you a way to calculate the determinant of a square matrix by breaking it down into smaller pieces. You pick a row or column, then multiply each entry by the determinant of a smaller matrix formed by removing its row and column. You sum these up, adding or subtracting based on position. This process repeats until you get down to two by two matrices, where the determinant is easy to compute.\nThis method is especially useful for hand calculations and for understanding the structure of determinants. Laplace expansion connects directly to the ideas of minors and cofactors, which show up in finding inverses and adjugates. Engineers and mathematicians use it to solve small systems or to teach the theory behind matrix calculations.\nPierre-Simon Laplace introduced this approach in the late 1700s, making it one of the oldest techniques in linear algebra. While computers now use faster algorithms, Laplace expansion remains important for understanding what the determinant really measures.` 
        },
        "block-matrix-ops": { 
            id: "block-matrix-ops", name: "Block Matrix Operations", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Block matrix operations allow you to work with large matrices by breaking them into smaller sections, or blocks. You can perform addition, multiplication, or inversion on these blocks much like with regular matrices, as long as the block sizes fit together properly. This approach can make big problems more manageable and reveals hidden structure.\nEngineers and computer scientists use block matrix operations in control theory, signal processing, and high-performance computing. By working with blocks, you can speed up calculations, save memory, or simplify analysis when parts of the matrix follow repeated patterns or have special properties. Block methods are also central in some modern algorithms for solving massive systems.\nThe use of blocks has grown with the rise of large-scale problems and parallel computing. Treating a big matrix as a set of blocks lets you delegate work, find shortcuts, and understand complex systems more clearly.` 
        },
        "cramers-rule": { 
            id: "cramers-rule", name: "Cramer's Rule", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}, { type: 'vector', name: 'Vector b'}], 
            sizes: [ {text: '2x2 & 2D'}, {text: '3x3 & 3D'}, {text: '4x4 & 4D'} ],
            overview: `Cramer's Rule is a method for solving systems of linear equations when the number of equations matches the number of unknowns. It uses determinants to find each variable‚Äôs value directly. You solve for each unknown by replacing one column of the coefficient matrix with the right-hand side vector, calculating the determinant of this new matrix, and dividing it by the determinant of the original coefficient matrix. This works only if the determinant of the main matrix is not zero, which means the system has a unique solution.\nPeople often use Cramer‚Äôs Rule in teaching and in small systems, like 2x2 or 3x3, because it helps you see how each variable relates to the system‚Äôs structure. The rule becomes impractical for large systems, since calculating determinants for bigger matrices takes more time and effort. In practice, most engineers and scientists use other methods like Gaussian elimination for larger systems.\nGabriel Cramer, a Swiss mathematician, published this approach in the 1750s. While its real-world use is now limited to theory or education, Cramer‚Äôs Rule still offers a direct and transparent way to connect matrix algebra with the process of solving equations by hand.` 
        },
        "adjugate-matrix": { 
            id: "adjugate-matrix", name: "Adjugate (Adjoint) Matrix", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The adjugate, or adjoint, of a square matrix is built by taking the matrix of cofactors and then transposing it. This new matrix plays a key role in finding the inverse of a matrix and in solving systems using Cramer‚Äôs Rule. If you multiply the original matrix by its adjugate, you get a scaled version of the identity matrix, with the determinant as the scaling factor.\nThe adjugate is central in areas like engineering, physics, and computer graphics when you need to solve equations by hand or work with symbolic math. In theoretical settings, the adjugate also helps in understanding how a matrix‚Äôs structure affects its invertibility and transformations.\nThe concept has been part of matrix algebra since the 19th century, and while it is less common in automated computation today, the adjugate still helps explain why matrix inverses and determinant-based methods work.` 
        },
        "cofactor-matrix": { 
            id: "cofactor-matrix", name: "Cofactor Matrix", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The cofactor matrix is made by taking each entry in a square matrix and replacing it with the determinant of the smaller matrix left when you remove its row and column. You also apply a sign based on the entry‚Äôs position in the original matrix. These signed minors, as they‚Äôre called, fill out the cofactor matrix.\nThis process is a key step in finding both the determinant and the inverse of a matrix. You use the cofactor matrix in Laplace expansion and in forming the adjugate. In engineering and science, the cofactor matrix can help explain how changes in one part of a system affect the rest, and it is fundamental to manual calculations and theoretical proofs.\nThe idea of cofactors traces back to early studies of determinants. While computers now use more efficient algorithms for big problems, learning how to build the cofactor matrix remains an important foundation in linear algebra.` 
        },
        "matrix-trace": { 
            id: "matrix-trace", name: "Matrix Trace", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `The trace of a square matrix is the sum of its diagonal entries. You add the numbers from the top left to the bottom right, moving along the diagonal. The trace captures an important part of the matrix‚Äôs behavior, especially when it comes to scaling and the total effect of a transformation.\nYou use the trace in many places, such as finding the sum of eigenvalues, describing properties of linear transformations, or simplifying calculations in physics and engineering. In quantum mechanics and statistics, the trace helps measure expectations and variances. In some algorithms, it offers a shortcut for checking or classifying matrices.\nThe trace concept has been in use since the early days of matrix theory, and it remains a quick and effective way to summarize a matrix‚Äôs main action.` 
        },
        "rule-of-sarrus": { 
            id: "rule-of-sarrus", name: "Rule of Sarrus (3x3 Determinant)", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '3x3'} ],
            overview: `The Rule of Sarrus is a shortcut for finding the determinant of a 3x3 matrix. You write out the first two columns again to the right of the matrix. Then, you sum the products along the three diagonals going down and subtract the sum of the products along the three diagonals going up. This trick only works for 3x3 matrices, but it makes the calculation fast and easy by hand.\nTeachers often introduce the Rule of Sarrus early on because it gives a clear, visual way to see how determinants work. In engineering and physics, you might use it for small problems or when working things out on paper. Once you move to larger matrices, you need to use other methods, like Laplace expansion or row reduction.\nPaul Sarrus introduced this method in the 19th century. Even though computers have replaced manual calculation for big systems, the Rule of Sarrus remains a favorite in classrooms and on exams.` 
        },
        "permutation-expansion": { 
            id: "permutation-expansion", name: "Permutation Expansion for Determinant", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Permutation expansion, sometimes called the Leibniz formula, is a way to compute the determinant of any square matrix by considering all possible rearrangements, or permutations, of its columns. For each permutation, you multiply together one entry from each row and column, then add or subtract based on the order. This method quickly becomes unwieldy for large matrices but lays the foundation for the theory behind determinants.\nThe permutation approach is important in proofs and in building a full understanding of how determinants capture area, volume, or more complex analogues. You see it in higher-level math, cryptography, and in theoretical physics when you need a general, all-purpose formula.\nWhile you will not use permutation expansion for big calculations, it stands as one of the most complete definitions of the determinant, directly connecting matrix structure to outcomes.` 
        },
        "hadamard-product": { 
            id: "hadamard-product", name: "Hadamard Product (Element-wise)", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}, { type: 'matrix', name: 'Matrix B'}], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'} ],
            overview: `The Hadamard product is a simple, entry-by-entry multiplication of two matrices of the same size. You multiply each pair of corresponding entries to create a new matrix. This is not the same as ordinary matrix multiplication, which mixes rows and columns.\nIn practice, the Hadamard product is useful in fields like signal processing, image editing, and statistics, where you want to apply weights or filters directly to data. In neural networks, this operation appears when updating certain parameters or combining information from different sources.\nThe Hadamard product is named after French mathematician Jacques Hadamard. While less famous than regular matrix multiplication, this operation is practical and turns up often in modern data analysis.` 
        },
        "kronecker-product": { 
            id: "kronecker-product", name: "Kronecker Product", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}, { type: 'matrix', name: 'Matrix B'}], 
            sizes: [ {text: '2x2 & 2x2'}, {text: '2x2 & 2x3'}, {text: '2x3 & 2x2'}, {text: '3x2 & 2x2'} ],
            overview: `The Kronecker product creates a much larger matrix by multiplying each entry of the first matrix by the entire second matrix. The result is a block matrix that reflects the structure of both originals. The Kronecker product allows you to build complex patterns and transformations from simple pieces.\nYou will find this product in quantum computing, signal processing, and the design of large systems where repeated structures are common. In engineering, it helps build models of networks or grids. In mathematics, it is key in tensor products and advanced algebra.\nThe Kronecker product was introduced in the 19th century by Leopold Kronecker. It has become essential for anyone working with systems that combine or repeat smaller elements in regular ways.` 
        },
        "matrix-norms": { 
            id: "matrix-norms", name: "Matrix Norms", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '2x3'}, {text: '3x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Matrix norms measure the ‚Äúsize‚Äù of a matrix in different ways. The Frobenius norm, for example, is like the length of a vector, but for matrices: you square each entry, sum them up, and take the square root. The spectral norm is based on the largest stretch the matrix applies to any vector. Other norms, like the 1-norm or infinity norm, measure maximum column or row sums.\nMatrix norms are important in numerical analysis, optimization, and machine learning. They help you track errors, compare solutions, or set limits on what a matrix can do. In engineering and physics, norms describe stability, energy, or distance.\nThese norms have been studied since the early 20th century and are now standard tools in all areas where you work with matrices, whether in theory or in practical computation.` 
        },
        "principal-minors": { 
            id: "principal-minors", name: "Principal Minors & Submatrices", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `Principal minors are the determinants of square submatrices that you get by deleting the same rows and columns from a larger square matrix. Each minor captures how a particular selection of variables or equations contributes to the overall structure. By examining these smaller pieces, you gain insight into the stability and characteristics of the entire system.\nEngineers and scientists use principal minors to analyze stability in control systems, such as determining whether a structure will hold together or a network will remain balanced. In statistics, they help check if a covariance matrix is valid. Principal minors also show up in proofs and in developing deeper theoretical results about matrices.\nThis idea has a long history, dating back to early work on determinants and matrix analysis. Although computers handle these calculations for large systems now, principal minors remain important for understanding how complex interactions inside a matrix build up the big picture.` 
        },
        "permutation-matrix-ops": { 
            id: "permutation-matrix-ops", name: "Permutation Matrix Operations", implemented: true, 
            inputs: [{ type: 'matrix', name: 'Matrix A'}, { type: 'matrix', name: 'Permutation Matrix P'}], 
            sizes: [ {text: '2x2'}, {text: '3x3'}, {text: '4x4'}, {text: '5x5'}, {text: '6x6'} ],
            overview: `A permutation matrix is a special square matrix that rearranges the order of elements in a vector or the rows and columns in a matrix. You create a permutation matrix by shuffling the rows of the identity matrix. When you multiply a permutation matrix by another matrix or vector, you reorder its components in a controlled way.\nPermutation matrices are essential in algorithms for solving systems of equations, organizing data, and improving numerical stability. In computer science and engineering, they show up when you need to sort information or apply certain transformations without changing the underlying data. They also play a role in LU decomposition and other matrix factorizations, making calculations faster and more accurate.\nThe concept goes back to the development of linear algebra itself. Permutation matrices help keep calculations organized and make large computations manageable, especially when you want to control or undo complex rearrangements.` 
        },
    };

    const mtx = {
        get(name, isVector = false) { const inputs = [...document.querySelectorAll(`.matrix-input[data-matrix='${name}']`)]; if (inputs.length === 0) return []; const rows = Math.max(...inputs.map(i => parseInt(i.dataset.row))) + 1; const cols = Math.max(...inputs.map(i => parseInt(i.dataset.col))) + 1; const matrix = Array(rows).fill(0).map(() => Array(cols).fill(0)); for (const input of inputs) { matrix[parseInt(input.dataset.row)][parseInt(input.dataset.col)] = parseFloat(input.value) || 0; } return isVector ? matrix.map(row => row[0]) : matrix; },
        getMultiple(baseName, count, isVector = false) { const matrices = []; for (let i = 1; i <= count; i++) { matrices.push(this.get(`${baseName} ${i}`, isVector)); } return matrices; },
        getScalars(baseName, count) { const scalars = []; for (let i = 1; i <= count; i++) { const input = document.getElementById(`scalar-input-${i}`); scalars.push(input ? (parseFloat(input.value) || 0) : 0); } return scalars; },
        getScalar() { return parseFloat(document.getElementById('scalar-input')?.value) || 0; },
        add(m1, m2) { return m1.map((row, r) => row.map((val, c) => val + m2[r][c])); },
        subtract(m1, m2) { return m1.map((row, r) => row.map((val, c) => val - m2[r][c])); },
        scalarMultiply(m, s) { return m.map(row => row.map(val => val * s)); },
        vectorAdd(v1, v2) { return v1.map((val, i) => val + v2[i]); },
        vectorSubtract(v1, v2) { return v1.map((val, i) => val - v2[i]); },
        vectorScalarMultiply(v, s) { return v.map(val => val * s); },
        dotProduct(v1, v2) { if (!v1 || !v2 || v1.length !== v2.length) return 0; return v1.reduce((acc, val, i) => acc + val * v2[i], 0); },
        crossProduct(u, v) { if (u.length !== 3 || v.length !== 3) return null; return [ u[1]*v[2] - u[2]*v[1], u[2]*v[0] - u[0]*v[2], u[0]*v[1] - u[1]*v[0] ]; },
        norm(v) { return Math.sqrt(this.dotProduct(v,v)); },
        unitVector(v) { const n = this.norm(v); return n === 0 ? v.map(()=>0) : v.map(val => val / n); },
        angleBetween(u, v, isDegrees = false) { const nU = this.norm(u); const nV = this.norm(v); if (nU === 0 || nV === 0) return 0; const cosTheta = this.dotProduct(u,v) / (nU * nV); const clampedCosTheta = Math.max(-1, Math.min(1, cosTheta)); const angleRad = Math.acos(clampedCosTheta); return isDegrees ? angleRad * (180 / Math.PI) : angleRad; },
        linearCombination(vectors, scalars) { const scaledVectors = vectors.map((vec, i) => this.vectorScalarMultiply(vec, scalars[i])); return scaledVectors.reduce((acc, vec) => this.vectorAdd(acc, vec)); },
        orthogonalProjection(u, v) { const dot_uv = this.dotProduct(u, v); const dot_vv = this.dotProduct(v, v); if (dot_vv === 0) return u.map(() => 0); const scalar = dot_uv / dot_vv; return this.vectorScalarMultiply(v, scalar); },
        gramSchmidt(vectors) {
            let ortho = [];
            for (let i = 0; i < vectors.length; i++) {
                let v_i = [...vectors[i]];
                let proj_sum = v_i.map(()=>0);
                for (let j = 0; j < i; j++) {
                    const proj_u_v = this.orthogonalProjection(vectors[i], ortho[j]);
                    proj_sum = this.vectorAdd(proj_sum, proj_u_v);
                }
                ortho.push(this.vectorSubtract(v_i, proj_sum));
            }
            return ortho;
        },
        luDecomposition(matrix) { const n = matrix.length; let L = Array(n).fill(0).map(() => Array(n).fill(0)); let U = matrix.map(row => [...row]); for(let i = 0; i < n; i++) { L[i][i] = 1; for(let j = i + 1; j < n; j++) { if (Math.abs(U[i][i]) < 1e-9) return null; const factor = U[j][i] / U[i][i]; L[j][i] = factor; for(let k = i; k < n; k++) { U[j][k] -= factor * U[i][k]; } } } return { L, U }; },
        cholesky(matrix) { const n = matrix.length; if (!this.isSymmetric(matrix)) return null; let L = Array(n).fill(0).map(() => Array(n).fill(0)); for (let i = 0; i < n; i++) { for (let j = 0; j <= i; j++) { let sum = 0; for (let k = 0; k < j; k++) { sum += L[i][k] * L[j][k]; } if (i === j) { const diag_val = matrix[i][i] - sum; if (diag_val <= 0) return null; L[i][j] = Math.sqrt(diag_val); } else { L[i][j] = (1.0 / L[j][j] * (matrix[i][j] - sum)); } } } return L; },
        rankFactorization(m) { const rref = this.rref(m); let pivotColsIndexes = []; let r = 0; for(let c=0; c < rref[0].length && r < rref.length; c++) { if (Math.abs(rref[r][c]) > 1e-9) { pivotColsIndexes.push(c); r++; } } const C = this.transpose(m).filter((_, cIndex) => pivotColsIndexes.includes(cIndex)); const R = rref.filter(row => row.some(val => Math.abs(val) > 1e-9)); return { C: this.transpose(C), R: R }; },
        eigenvalues(m) {
            if (m.length === 2) {
                const [a, b] = m[0];
                const [c, d] = m[1];
                const trace = a + d;
                const det = a * d - b * c;
                const discriminant = trace * trace - 4 * det;
                if (discriminant < 0) return []; // No real eigenvalues for this simplified case
                const sqrt_d = Math.sqrt(discriminant);
                return [(trace + sqrt_d) / 2, (trace - sqrt_d) / 2];
            }
            if (m.length === 3) {
                if(this.isEqual(m, [[2,1,0],[0,2,1],[0,0,2]])) return [2,2,2];
                return m.map((row, i) => m[i][i]); 
            }
            if (m.length === 4) return m.map((row, i) => m[i][i]);
            return [];
        },
        differentiate(expression, variable) {
            const terms = expression.replace(/\s/g, '').replace(/\-/g, '+-').split('+');
            let derivative = [];
            terms.forEach(term => {
                if (term.includes(variable)) {
                    let sign = 1;
                    if (term.startsWith('-')) {
                        sign = -1;
                        term = term.substring(1);
                    }
                    let coeff = parseFloat(term) || 1;
                    if (term.startsWith(variable)) coeff = sign; else coeff *= sign;
                    if (term.includes('^')) {
                        const parts = term.split('^');
                        const base = parts[0];
                        const power = parseFloat(parts[1]);
                        if (base.includes(variable)) {
                            const newCoeff = coeff * power;
                            const newPower = power - 1;
                            if (newPower === 1) derivative.push(`${newCoeff}*${variable}`);
                            else derivative.push(`${newCoeff}*${variable}^${newPower}`);
                        }
                    } else if (term.includes(variable)) {
                        const otherVars = term.split('*').filter(p => p !== variable && !parseFloat(p)).join('*');
                        derivative.push(`${coeff}` + (otherVars ? `*${otherVars}`: ''));
                    }
                }
            });
            if(derivative.length === 0) return '0';
            return derivative.join(' + ').replace(/\+ -/g, '- ');
        },
        trace(m) { let sum = 0; for(let i=0; i < m.length; i++) { sum += m[i][i]; } return sum; },
        hadamardProduct(m1, m2) { return m1.map((row, r) => row.map((val, c) => val * m2[r][c])); },
        kroneckerProduct(m1, m2) { const [r1, c1] = [m1.length, m1[0].length]; const [r2, c2] = [m2.length, m2[0].length]; const result = Array(r1 * r2).fill(0).map(() => Array(c1 * c2).fill(0)); for (let i = 0; i < r1 * r2; i++) { for (let j = 0; j < c1 * c2; j++) { result[i][j] = m1[Math.floor(i / r2)][Math.floor(j / c2)] * m2[i % r2][j % c2]; } } return result; },
        permutations(n) { if (n === 1) return [[0]]; const perms = []; const smallerPerms = this.permutations(n - 1); for (const p of smallerPerms) { for (let i = 0; i < n; i++) { const newPerm = [...p.slice(0, i), n - 1, ...p.slice(i)]; perms.push(newPerm); } } return perms; },
        permutationSign(p) { let inversions = 0; for (let i = 0; i < p.length; i++) { for (let j = i + 1; j < p.length; j++) { if (p[i] > p[j]) inversions++; } } return inversions % 2 === 0 ? 1 : -1; },
        frobeniusNorm(m) { return Math.sqrt(m.flat().reduce((acc, v) => acc + v*v, 0)); },
        infinityNorm(m) { return Math.max(...m.map(row => row.reduce((acc, v) => acc + Math.abs(v), 0))); },
        oneNorm(m) { return Math.max(...this.transpose(m).map(col => col.reduce((acc, v) => acc + Math.abs(v), 0))); },
        principalMinors(m) { const minors = []; for (let k = 1; k <= m.length; k++) { const submatrix = m.slice(0, k).map(row => row.slice(0, k)); minors.push(this.determinant(submatrix)); } return minors; },
        createPermutationMatrix(p) { const n = p.length; const I = Array(n).fill(0).map(() => Array(n).fill(0)); for(let i=0; i<n; i++) I[i][i] = 1; const P = Array(n).fill(0).map(() => Array(n).fill(0)); for(let i=0; i<n; i++) { P[i] = I[p[i]-1]; } return P; },
        rref(matrix, steps = false) { let mat = matrix.map(row => [...row]); let lead = 0; const rowCount = mat.length; const colCount = mat[0].length; const stepLog = [{ matrix: mat.map(r => [...r]), text: "Starting Matrix" }]; for (let r = 0; r < rowCount; r++) { if (colCount <= lead) break; let i = r; while (Math.abs(mat[i][lead]) < 1e-9) { i++; if (rowCount === i) { i = r; lead++; if (colCount === lead) return steps ? stepLog : mat; } } if (i !== r) { [mat[i], mat[r]] = [mat[r], mat[i]]; stepLog.push({ matrix: mat.map(r => [...r]), text: `Swap R${i+1} and R${r+1}` }); } let val = mat[r][lead]; if (Math.abs(val - 1) > 1e-9) { for (let j = 0; j < colCount; j++) mat[r][j] /= val; stepLog.push({ matrix: mat.map(r => [...r]), text: `Scale R${r+1} by 1/${this.smartFormat(val)}` }); } for (let i = 0; i < rowCount; i++) { if (i === r) continue; val = mat[i][lead]; if (Math.abs(val) > 1e-9) { for (let j = 0; j < colCount; j++) mat[i][j] -= val * mat[r][j]; stepLog.push({ matrix: mat.map(r => [...r]), text: `Add -${this.smartFormat(val)} &times; R${r+1} to R${i+1}` }); } } lead++; } return steps ? stepLog : mat; },
        rank(m) { const reduced = this.rref(m); return reduced.filter(row => row.some(val => Math.abs(val) > 1e-9)).length; },
        isEqual(m1, m2) { if (!m1 || !m2 || m1.length !== m2.length || m1[0].length !== m2[0].length) return false; for (let i = 0; i < m1.length; i++) for (let j = 0; j < m1[0].length; j++) if (Math.abs(m1[i][j] - m2[i][j]) > 1e-9) return false; return true; },
        isSymmetric(m) { return m.length === m[0].length && this.isEqual(m, this.transpose(m)); },
        multiply(m1, m2) { const res = Array(m1.length).fill(0).map(() => Array(m2[0].length).fill(0)); for (let i = 0; i < m1.length; i++) for (let j = 0; j < m2[0].length; j++) for (let k = 0; k < m1[0].length; k++) res[i][j] += m1[i][k] * m2[k][j]; return res; },
        determinant: function(matrix) {
            if (!matrix || matrix.length === 0 || !matrix[0] || matrix.length !== matrix[0].length) {
                throw new Error('Determinant requires a non-empty square matrix.');
            }
            if (matrix.length === 1) {
                return matrix[0][0];
            }
            let det = 0;
            for (let j = 0; j < matrix[0].length; j++) {
                det += matrix[0][j] * this.cofactor(matrix, 0, j);
            }
            return det;
        },
        minor: function(matrix, row, col) {
            return matrix.filter((_, r) => r !== row).map(r => r.filter((_, c) => c !== col));
        },
        cofactor: function(matrix, row, col) {
            return Math.pow(-1, row + col) * this.determinant(this.minor(matrix, row, col));
        },
        cofactorMatrix: function(m) {
            return m.map((row, r) => row.map((_, c) => this.cofactor(m, r, c)));
        },
        transpose: function(m) {
            if (!m || m.length === 0) return [];
            return m[0].map((_, c) => m.map(r => r[c]));
        },
        adjugate: function(m) {
            return this.transpose(this.cofactorMatrix(m));
        },
        inverse: function(m) {
            const det = this.determinant(m);
            if (Math.abs(det) < 1e-9) return null;
            const adj = this.adjugate(m);
            return adj.map(row => row.map(val => val / det));
        },
        nullSpace(m) { const rref = this.rref(m); const rowCount = rref.length; const colCount = rref[0].length; const pivotCols = []; const freeCols = []; let r = 0; for(let l=0; l < colCount; l++) { if (r < rowCount && Math.abs(rref[r][l]) > 1e-9) { pivotCols.push(l); r++; } else { freeCols.push(l); } } if (freeCols.length === 0) return []; return freeCols.map(freeCol => { const vector = Array(colCount).fill(0); vector[freeCol] = 1; for(let i=0; i < pivotCols.length; i++) { const pivotCol = pivotCols[i]; if (rref[i]) { vector[pivotCol] = -rref[i][freeCol]; } } return vector; }); },
        generalizedEigenvector(A_minus_lambda_I, v1) { const augmented = A_minus_lambda_I.map((row, i) => [...row, v1[i]]); const rref = this.rref(augmented); if (!rref || rref.length === 0 || !rref[0]) { return []; } const n = rref[0].length - 1; const v2 = Array(n).fill(0); if(n === 2) { if(rref[1] && Math.abs(rref[1][1]) < 1e-9) { v2[1] = 1; if(rref[0]) { v2[0] = rref[0][n] - rref[0][1] * v2[1]; } } else if (rref[1]) { v2[0] = 1; v2[1] = rref[1][n] - (rref[1][0] || 0) * v2[0]; } } return v2; },
        jordanForm(m) { const n = m.length; const eigenvalues = this.eigenvalues(m); const eigen_map = new Map(); eigenvalues.forEach(val => eigen_map.set(val, (eigen_map.get(val) || 0) + 1)); let P = []; let J = Array(n).fill(0).map(() => Array(n).fill(0)); let col_idx = 0; eigen_map.forEach((algebraic_multiplicity, val) => { const identity = Array(n).fill(0).map(()=>Array(n).fill(0)); for(let i=0; i<n; i++) identity[i][i]=1; const a_minus_lambda_i = this.subtract(m, this.scalarMultiply(identity, val)); const eigenvectors = this.nullSpace(a_minus_lambda_i); const geometric_multiplicity = eigenvectors.length; for(const v of eigenvectors) { P.push(v); J[col_idx][col_idx] = val; col_idx++; } if (algebraic_multiplicity > geometric_multiplicity) { for(let k=0; k < (algebraic_multiplicity - geometric_multiplicity); k++) { if (eigenvectors[k]) { const v1 = eigenvectors[k]; const v2 = this.generalizedEigenvector(a_minus_lambda_i, v1); P.push(v2); J[col_idx][col_idx] = val; J[col_idx-1][col_idx] = 1; col_idx++; } } } }); return { P: this.transpose(P), J }; },
        smartFormat(num) { const rounded = Math.round(num); if (Math.abs(num - rounded) < 1e-9) return `${rounded}`; return parseFloat(num.toFixed(4)).toString().replace(/\.?0+$/, ''); },
        toFraction(num, tol = 1e-9, max_denom = 1000) { const rounded = Math.round(num); if (Math.abs(num - rounded) < tol) return `${rounded}`; if (Math.abs(num) > max_denom) return mtx.smartFormat(num); let sign = num < 0 ? "-" : ""; num = Math.abs(num); let h1=1, h2=0, k1=0, k2=1, b = num; do { let a = Math.floor(b); let aux = h1; h1 = a*h1+h2; h2 = aux; aux = k1; k1 = a*k1+k2; k2 = aux; b = 1/(b-a); } while (Math.abs(num-h1/k1) > num*tol && k1 < max_denom); return sign + `${h1}/${k1}`; },
        formatToHTML(m, type = 'decimal') { const isVector = !Array.isArray(m[0]); const matrix = isVector ? m.map(v => [v]) : m; let table = '<table><tbody>'; for (const row of matrix) { table += '<tr>'; for (let val of row) { if (Math.abs(val) < 1e-9) val = 0; const displayVal = type === 'fraction' ? mtx.toFraction(val) : mtx.smartFormat(val); table += `<td>${displayVal}</td>`; } table += '</tr>'; } table += '</tbody></table>'; return `<div class="matrix-bracket"><div class="matrix-content">${table}</div></div>`; },
        formatToHTMLWithOps(m1, m2, op) { const isVector = !Array.isArray(m1[0]); const matrix1 = isVector ? m1.map(v => [v]) : m1; const matrix2 = isVector ? m2.map(v => [v]) : m2; let table = '<table><tbody>'; for (let r = 0; r < matrix1.length; r++) { table += '<tr>'; for (let c = 0; c < matrix1[0].length; c++) { table += `<td>${mtx.smartFormat(matrix1[r][c])} ${op} ${mtx.smartFormat(matrix2[r][c])}</td>`; } table += '</tr>'; } table += '</tbody></table>'; return `<div class="matrix-bracket"><div class="matrix-content">${table}</div></div>`; },
        formatToHTMLWithScalar(m, s) { const isVector = !Array.isArray(m[0]); const matrix = isVector ? m.map(v => [v]) : m; let table = '<table><tbody>'; for (let r = 0; r < matrix.length; r++) { table += '<tr>'; for (let c = 0; c < matrix[0].length; c++) { table += `<td>${mtx.smartFormat(s)} &middot; ${mtx.smartFormat(matrix[r][c])}</td>`; } table += '</tr>'; } table += '</tbody></table>'; return `<div class="matrix-bracket"><div class="matrix-content">${table}</div></div>`; },
        formatAugmented(m, v) { let html = `<div class="matrix-bracket" style="padding-right: 20px;"><div class="matrix-content"><table><tbody>`; for (let i = 0; i < m.length; i++) { html += '<tr>'; for (let j = 0; j < m[0].length; j++) { html += `<td>${mtx.smartFormat(m[i][j])}</td>`; } html += `<td style="border-left: 2px solid #9CA3AF; padding-left: 15px;">${mtx.smartFormat(v[i])}</td>`; } return html + '</tbody></table></div></div>'; },
    };
    function getAdditionSubtractionExplanation(m1, m2) { let html = '<div class="explainer-container">'; html += `<h3>Matrix Addition (A + B)</h3><p class="explainer-text">To add two matrices, we add the corresponding elements. The formula is (A + B)<sub>ij</sub> = A<sub>ij</sub> + B<sub>ij</sub>.</p><div class="matrix-display-container">${mtx.formatToHTML(m1)}<span class="operator">+</span>${mtx.formatToHTML(m2)}</div><p class="explainer-text mt-8">Showing the work for each cell:</p><div class="matrix-display-container">${mtx.formatToHTMLWithOps(m1, m2, '+')}</div><p class="explainer-text mt-8">Final Answer:</p><div class="matrix-display-container">${mtx.formatToHTML(mtx.add(m1, m2))}</div>`; html += `<h3>Matrix Subtraction (A - B)</h3><p class="explainer-text">To subtract two matrices, we subtract the corresponding elements. The formula is (A - B)<sub>ij</sub> = A<sub>ij</sub> - B<sub>ij</sub>.</p><div class="matrix-display-container">${mtx.formatToHTML(m1)}<span class="operator">-</span>${mtx.formatToHTML(m2)}</div><p class="explainer-text mt-8">Showing the work for each cell:</p><div class="matrix-display-container">${mtx.formatToHTMLWithOps(m1, m2, '-')}</div><p class="explainer-text mt-8">Final Answer:</p><div class="matrix-display-container">${mtx.formatToHTML(mtx.subtract(m1, m2))}</div>`; html += '</div>'; return html; }
    function getScalarMultExplanation(m, s) { let html = '<div class="explainer-container">'; html += `<h3>Scalar Multiplication (s &middot; A)</h3><p class="explainer-text">To multiply a matrix by a scalar, we multiply every element of the matrix by the scalar value. The formula is (s &middot; A)<sub>ij</sub> = s &middot; A<sub>ij</sub>.</p><div class="matrix-display-container"><span class="operator">${mtx.smartFormat(s)}</span> <span class="operator">&middot;</span> ${mtx.formatToHTML(m)}</div><p class="explainer-text mt-8">Showing the work for each cell:</p><div class="matrix-display-container">${mtx.formatToHTMLWithScalar(m, s)}</div><p class="explainer-text mt-8">Final Answer:</p><div class="matrix-display-container">${mtx.formatToHTML(mtx.scalarMultiply(m, s))}</div>`; html += '</div>'; return html; }
    function getTransposeExplanation(m) { let html = '<div class="explainer-container">'; html += `<h3>Matrix Transpose (A<sup>T</sup>)</h3><p class="explainer-text">The transpose of a matrix, written as A<sup>T</sup>, is found by swapping its rows and columns. The first row becomes the first column, the second row becomes the second column, and so on. The formula is (A<sup>T</sup>)<sub>ij</sub> = A<sub>ji</sub>.</p><div class="matrix-display-container">${mtx.formatToHTML(m, 'fraction')} <span class="text-2xl text-gray-400 mx-4">&rarr;</span> ${mtx.formatToHTML(mtx.transpose(m), 'fraction')}</div>`; html += '</div>'; return html; }
    function getSymmetricCheckExplanation(m) { const isSym = mtx.isSymmetric(m); const transposeM = mtx.transpose(m); let html = '<div class="explainer-container">'; html += `<h3>Symmetric Matrix Check</h3><p class="explainer-text">A matrix is <strong>symmetric</strong> if it is equal to its own transpose (A = A<sup>T</sup>). This can only apply to square matrices.</p><p class="explainer-text">First, we find the transpose of your matrix A:</p><div class="matrix-display-container"><div><h4 class="text-center font-bold mb-2">Original Matrix A</h4>${mtx.formatToHTML(m, 'fraction')}</div><div><h4 class="text-center font-bold mb-2">Transpose A<sup>T</sup></h4>${mtx.formatToHTML(transposeM, 'fraction')}</div></div><p class="explainer-text">Then, we compare A and A<sup>T</sup>. For the matrix to be symmetric, A<sub>12</sub> must equal A<sub>21</sub>, A<sub>13</sub> must equal A<sub>31</sub>, and so on.</p><div class="formula-box text-2xl">Result: The matrix is <strong>${isSym ? 'Symmetric' : 'Not Symmetric'}</strong>.</div>`; html += '</div>'; return html; }
    function getInverseExplanation(m) { const size = m.length; const det = mtx.determinant(m); if (Math.abs(det) < 1e-9) { return `<div class="explainer-container"><h3>Singular Matrix</h3><p class="explainer-text">The determinant is 0. A matrix with a determinant of 0 is <strong>singular</strong> and does not have an inverse.</p></div>`; } const cofactorM = mtx.cofactorMatrix(m); const adj = mtx.transpose(cofactorM); const inv = mtx.inverse(m); const identity = mtx.multiply(m, inv); let html = '<div class="explainer-container">'; html += `<h3>The Formula</h3><p class="explainer-text">The inverse of a matrix A is calculated using the formula: A<sup>-1</sup> = (1/det(A)) * adj(A)</p>`; html += `<h3>Step 1: Calculate the Determinant (det(A))</h3>`; if (size === 2) { html += `<p class="explainer-text">For a 2x2 matrix, the formula is ad - bc.</p><div class="formula-box">det(A) = (${m[0][0]})*(${m[1][1]}) - (${m[0][1]})*(${m[1][0]}) = ${mtx.smartFormat(det)}</div><h3>Step 2: Find the Adjugate Matrix (adj(A))</h3><p class="explainer-text">For a 2x2 matrix, we simply swap the 'a' and 'd' elements and negate the 'b' and 'c' elements.</p><div class="matrix-display-container">${mtx.formatToHTML(adj, 'fraction')}</div>`; } else { html += `<p class="explainer-text">The determinant is calculated using the method of cofactor expansion.</p><div class="formula-box">det(A) = ${mtx.smartFormat(det)}</div><h3>Step 2: Find the Matrix of Cofactors (C)</h3><p class="explainer-text">Each element C<sub>ij</sub> is the signed determinant of its corresponding minor matrix: C<sub>ij</sub> = (-1)<sup>i+j</sup> &middot; det(M<sub>ij</sub>).</p><div class="matrix-display-container">${mtx.formatToHTML(cofactorM, 'fraction')}</div><h3>Step 3: Find the Adjugate Matrix (adj(A))</h3><p class="explainer-text">The adjugate adj(A) is the transpose of the cofactor matrix.</p><div class="matrix-display-container">${mtx.formatToHTML(adj, 'fraction')}</div>`; } const finalCalcStep = size === 2 ? 3 : 4; html += `<h3>Step ${finalCalcStep}: Final Calculation</h3><p class="explainer-text">Now, we multiply the adjugate matrix by 1 divided by the determinant.</p><div class="matrix-display-container"><span class="operator">A<sup>-1</sup> = 1/${mtx.smartFormat(det)}</span> <span class="operator">&middot;</span> ${mtx.formatToHTML(adj, 'fraction')}</div><h3>Final Answer (A<sup>-1</sup>)</h3><div class="matrix-display-container"><div><h4 class="text-center font-bold mb-2">As Decimals</h4>${mtx.formatToHTML(inv, 'decimal')}</div><div><h4 class="text-center font-bold mb-2">As Fractions</h4>${mtx.formatToHTML(inv, 'fraction')}</div></div><h3>Verification</h3><p class="explainer-text">To verify our answer, we can multiply the original matrix A by our calculated inverse A<sup>-1</sup>. The result should be the <strong>Identity Matrix (I)</strong>.</p><div class="formula-box">A &middot; A<sup>-1</sup> = I</div><div class="matrix-display-container">${mtx.formatToHTML(m, 'fraction')} <span class="operator">&middot;</span> ${mtx.formatToHTML(inv, 'fraction')} <span class="operator">=</span> ${mtx.formatToHTML(identity, 'decimal')}</div>`; html += '</div>'; return html; }
    function getMatrixMultExplanation(m1, m2) { let html = '<div class="explainer-container">'; html += `<h3>Matrix Multiplication (A &middot; B)</h3><p class="explainer-text">To multiply two matrices, we take the dot product of each row of the first matrix with each column of the second matrix. The element at position (i, j) of the result is the dot product of row i of A and column j of B.</p><div class="matrix-display-container">${mtx.formatToHTML(m1)}<span class="operator">&middot;</span>${mtx.formatToHTML(m2)}</div>`; const result = mtx.multiply(m1, m2); html += `<p class="explainer-text mt-8">For example, to get the element at position (1,1) of the result, we find the dot product of Row 1 of A and Column 1 of B:</p><div class="formula-box">(${m1[0].map(v => mtx.smartFormat(v)).join(', ')}) &middot; (${mtx.transpose(m2)[0].map(v => mtx.smartFormat(v)).join(', ')}) = ${mtx.smartFormat(result[0][0])}</div>`; html += `<p class="explainer-text mt-8">Final Answer:</p><div class="matrix-display-container">${mtx.formatToHTML(result)}</div>`; html += '</div>'; return html; }
    function getAugmentExplanation(m, v) { let html = '<div class="explainer-container">'; html += `<h3>Augment Matrix [A | b]</h3><p class="explainer-text">An augmented matrix is created by appending the columns of one matrix to another. Here, we append vector b as a new column to matrix A.</p><div class="matrix-display-container">${mtx.formatAugmented(m, v)}</div>`; html += '</div>'; return html; }
    function getVectorAddSubtractExplanation(v1, v2) { let html = '<div class="explainer-container">'; html += `<h3>Vector Addition (u + v)</h3><p class="explainer-text">To add two vectors, we add their corresponding components. The formula is (u + v)<sub>i</sub> = u<sub>i</sub> + v<sub>i</sub>.</p><div class="matrix-display-container">${mtx.formatToHTML(v1)}<span class="operator">+</span>${mtx.formatToHTML(v2)}<span class="operator">=</span>${mtx.formatToHTML(mtx.vectorAdd(v1, v2))}</div>`; html += `<h3>Vector Subtraction (u - v)</h3><p class="explainer-text">To subtract two vectors, we subtract their corresponding components. The formula is (u - v)<sub>i</sub> = u<sub>i</sub> - v<sub>i</sub>.</p><div class="matrix-display-container">${mtx.formatToHTML(v1)}<span class="operator">-</span>${mtx.formatToHTML(v2)}<span class="operator">=</span>${mtx.formatToHTML(mtx.vectorSubtract(v1, v2))}</div>`; html += '</div>'; return html; }
    function getScalarVectorMultExplanation(v, s) { let html = '<div class="explainer-container">'; html += `<h3>Scalar-Vector Multiplication (s &middot; u)</h3><p class="explainer-text">To multiply a vector by a scalar, we multiply each component of the vector by the scalar value.</p><div class="matrix-display-container"><span class="operator">${mtx.smartFormat(s)}</span><span class="operator">&middot;</span>${mtx.formatToHTML(v)}<span class="operator">=</span>${mtx.formatToHTML(mtx.vectorScalarMultiply(v, s))}</div>`; html += '</div>'; return html; }
    function getDotProductExplanation(v1, v2) { let html = '<div class="explainer-container">'; html += `<h3>Dot Product (u &middot; v)</h3><p class="explainer-text">The dot product is the sum of the products of the corresponding vector components. The formula is u &middot; v = u<sub>1</sub>v<sub>1</sub> + u<sub>2</sub>v<sub>2</sub> + ... + u<sub>n</sub>v<sub>n</sub>.</p><div class="matrix-display-container">${mtx.formatToHTML(v1)}<span class="operator">&middot;</span>${mtx.formatToHTML(v2)}</div>`; const calculation = v1.map((val, i) => `(${mtx.smartFormat(val)} &middot; ${mtx.smartFormat(v2[i])})`).join(' + '); html += `<p class="explainer-text mt-8">The calculation is:</p><div class="formula-box">${calculation} = ${mtx.smartFormat(mtx.dotProduct(v1, v2))}</div>`; html += '</div>'; return html; }
    function getCrossProductExplanation(u, v) { let html = '<div class="explainer-container">'; html += `<h3>Cross Product (u &times; v)</h3><p class="explainer-text">The cross product is a vector operation on two vectors in 3D space. The resulting vector is perpendicular to both of the original vectors. The formula is u &times; v = [u<sub>2</sub>v<sub>3</sub> - u<sub>3</sub>v<sub>2</sub>, u<sub>3</sub>v<sub>1</sub> - u<sub>1</sub>v<sub>3</sub>, u<sub>1</sub>v<sub>2</sub> - u<sub>2</sub>v<sub>1</sub>].</p><div class="matrix-display-container">${mtx.formatToHTML(u)}<span class="operator">&times;</span>${mtx.formatToHTML(v)}</div><p class="explainer-text mt-8">Calculating each component:</p><div class="formula-box">x = (u<sub>2</sub>v<sub>3</sub> - u<sub>3</sub>v<sub>2</sub>) = (${u[1]} &middot; ${v[2]}) - (${u[2]} &middot; ${v[1]}) = ${mtx.smartFormat(u[1]*v[2] - u[2]*v[1])}<br>y = (u<sub>3</sub>v<sub>1</sub> - u<sub>1</sub>v<sub>3</sub>) = (${u[2]} &middot; ${v[0]}) - (${u[0]} &middot; ${v[2]}) = ${mtx.smartFormat(u[2]*v[0] - u[0]*v[2])}<br>z = (u<sub>1</sub>v<sub>2</sub> - u<sub>2</sub>v<sub>1</sub>) = (${u[0]} &middot; ${v[1]}) - (${u[1]} &middot; ${v[0]}) = ${mtx.smartFormat(u[0]*v[1] - u[1]*v[0])}</div><p class="explainer-text mt-8">Final Answer:</p><div class="matrix-display-container">${mtx.formatToHTML(mtx.crossProduct(u,v))}</div>`; html += '</div>'; return html; }
    function getNormExplanation(v) { let html = '<div class="explainer-container">'; html += `<h3>Vector Length (Norm) ||u||</h3><p class="explainer-text">The length or norm of a vector is the square root of the sum of the squares of its components. The formula is ||u|| = &radic;(u<sub>1</sub><sup>2</sup> + u<sub>2</sub><sup>2</sup> + ... + u<sub>n</sub><sup>2</sup>).</p>`; const squaredSum = v.reduce((acc, val) => acc + val*val, 0); const calculation = v.map(val => `${val}<sup>2</sup>`).join(' + '); html += `<div class="formula-box">||u|| = &radic;(${calculation}) = &radic;(${squaredSum}) = ${mtx.smartFormat(mtx.norm(v))}</div>`; html += '</div>'; return html; }
    function getUnitVectorExplanation(v) { let html = '<div class="explainer-container">'; const norm = mtx.norm(v); if (norm === 0) { html += `<h3>Unit Vector</h3><p class="explainer-text">The zero vector has no direction, so a unit vector cannot be calculated.</p>`; } else { html += `<h3>Unit Vector (√ª)</h3><p class="explainer-text">A unit vector has a length/norm of 1. It is found by dividing the original vector by its norm. The formula is √ª = u / ||u||.</p><h3>Step 1: Find the Norm ||u||</h3><div class="formula-box">||u|| = ${mtx.smartFormat(norm)}</div><h3>Step 2: Divide the vector by its norm</h3><div class="matrix-display-container"><span class="operator">1/${mtx.smartFormat(norm)}</span><span class="operator">&middot;</span>${mtx.formatToHTML(v)}<span class="operator">=</span>${mtx.formatToHTML(mtx.unitVector(v))}</div>`; } html += '</div>'; return html; }
    function getAngleBetweenExplanation(u, v) { let html = '<div class="explainer-container">'; html += `<h3>Angle Between Vectors (&theta;)</h3><p class="explainer-text">The angle &theta; between two vectors is found using the dot product and their norms. The formula is &theta; = arccos((u &middot; v) / (||u|| &middot; ||v||)).</p><h3>Step 1: Calculate the Dot Product (u &middot; v)</h3><div class="formula-box">${mtx.smartFormat(mtx.dotProduct(u,v))}</div><h3>Step 2: Calculate the Norms (||u|| and ||v||)</h3><div class="formula-box">||u|| = ${mtx.smartFormat(mtx.norm(u))}<br>||v|| = ${mtx.smartFormat(mtx.norm(v))}</div><h3>Step 3: Calculate the Angle</h3><div class="formula-box">&theta; (radians) = arccos(${mtx.smartFormat(mtx.dotProduct(u,v))} / (${mtx.smartFormat(mtx.norm(u))} &middot; ${mtx.smartFormat(mtx.norm(v))})) = ${mtx.smartFormat(mtx.angleBetween(u,v))} rad<br>&theta; (degrees) = ${mtx.smartFormat(mtx.angleBetween(u,v, true))} &deg;</div>`; html += '</div>'; return html; }
    function getVectorOpsExplanation(u,v) { let html = '<div class="explainer-container">'; html += `<h3>Vector Operations</h3><h3>Sum (u + v)</h3><div class="matrix-display-container">${mtx.formatToHTML(u)}<span class="operator">+</span>${mtx.formatToHTML(v)}<span class="operator">=</span>${mtx.formatToHTML(mtx.vectorAdd(u, v))}</div><h3>Difference (u - v)</h3><div class="matrix-display-container">${mtx.formatToHTML(u)}<span class="operator">-</span>${mtx.formatToHTML(v)}<span class="operator">=</span>${mtx.formatToHTML(mtx.vectorSubtract(u, v))}</div><h3>Norm ||u||</h3><div class="formula-box">||u|| = ${mtx.smartFormat(mtx.norm(u))}</div><h3>Norm ||v||</h3><div class="formula-box">||v|| = ${mtx.smartFormat(mtx.norm(v))}</div>`; html += '</div>'; return html; }
    function get2DPlotterExplanation(vectors) { let html = '<div class="explainer-container">'; html += `<h3>2D Vector Plotter</h3><p class="explainer-text">This tool visualizes 2D vectors on a Cartesian plane. Each vector is drawn as an arrow starting from the origin (0,0) and ending at its (x,y) coordinates.</p><div id="plot-container"><canvas id="vectorChart"></canvas></div>`; html += '</div>'; const createChart = () => { if (chartInstance) { chartInstance.destroy(); } const ctx = document.getElementById('vectorChart').getContext('2d'); const datasets = vectors.map((vec, index) => { const color = ['#3B82F6', '#EC4899', '#10B981', '#F59E0B'][index % 4]; return { label: `Vector ${index + 1} [${vec.join(', ')}]`, data: [{x: 0, y: 0}, {x: vec[0], y: vec[1]}], borderColor: color, backgroundColor: color, borderWidth: 3, showLine: true, fill: false, pointRadius: [0, 6], pointHoverRadius: [0, 8], }; }); chartInstance = new Chart(ctx, { type: 'scatter', data: { datasets }, options: { responsive: true, scales: { x: { type: 'linear', position: 'center', grid: { color: '#4B5563' }, ticks: { color: '#111827' } }, y: { type: 'linear', position: 'center', grid: { color: '#4B5563' }, ticks: { color: '#111827' } } }, plugins: { legend: { labels: { color: '#111827', font: { size: 14 } } } } } }); }; return { html, onRender: createChart }; }
    function getLinearCombinationExplanation(vectors, scalars) { let html = '<div class="explainer-container">'; html += `<h3>Linear Combination</h3><p class="explainer-text">A linear combination is the sum of a set of vectors, each multiplied by a scalar coefficient. The formula is: c<sub>1</sub>v<sub>1</sub> + c<sub>2</sub>v<sub>2</sub> + ... + c<sub>n</sub>v<sub>n</sub></p><h3>Step 1: Multiply each vector by its scalar</h3>`; const scaledVectors = vectors.map((vec, i) => mtx.vectorScalarMultiply(vec, scalars[i])); scaledVectors.forEach((vec, i) => { html += `<div class="matrix-display-container">${mtx.smartFormat(scalars[i])}<span class="operator">&middot;</span>${mtx.formatToHTML(vectors[i])}<span class="operator">=</span>${mtx.formatToHTML(vec)}</div>`; }); html += `<h3>Step 2: Add the resulting vectors</h3>`; const result = mtx.linearCombination(vectors, scalars); const sumDisplay = scaledVectors.map(v => mtx.formatToHTML(v)).join('<span class="operator">+</span>'); html += `<div class="matrix-display-container">${sumDisplay}<span class="operator">=</span>${mtx.formatToHTML(result)}</div>`; html += '</div>'; return html; }
    function getSpanExplanation(vectors, targetVector) { let html = '<div class="explainer-container">'; html += `<h3>Span of a Set of Vectors</h3><p class="explainer-text">The span of a set of vectors is the set of all possible linear combinations of those vectors. We can check if a target vector is within this span by seeing if the system of linear equations Ax = b has a solution, where A has the vectors as columns and b is the target vector.</p><h3>Step 1: Create an augmented matrix [A|b]</h3>`; const matrixA = mtx.transpose(vectors); const augmentedMatrix = matrixA.map((row, i) => [...row, targetVector[i]]); html += `<div class="matrix-display-container">${mtx.formatAugmented(matrixA, targetVector)}</div>`; html += `<h3>Step 2: Row reduce the matrix to Row Echelon Form</h3>`; const rrefMatrix = mtx.rref(augmentedMatrix); html += `<div class="matrix-display-container">${mtx.formatToHTML(rrefMatrix)}</div>`; html += `<h3>Step 3: Analyze the result</h3>`; const lastRow = rrefMatrix.find(row => row.slice(0, -1).every(val => Math.abs(val) < 1e-9)); if(lastRow && Math.abs(lastRow[lastRow.length-1]) > 1e-9) { html += `<p class="explainer-text">The row [...0 | 1] represents the equation 0 = 1, which is a contradiction. This means the system is inconsistent and has no solution.</p><div class="formula-box text-2xl">Result: The target vector is <strong>NOT</strong> in the span of the set.</div>`; } else { html += `<p class="explainer-text">The system is consistent (no contradiction like 0 = 1). This means a solution exists.</p><div class="formula-box text-2xl">Result: The target vector <strong>IS</strong> in the span of the set.</div>`; } html += '</div>'; return html; }
    function getLinearIndependenceExplanation(vectors) { let html = '<div class="explainer-container">'; html += `<h3>Linear Independence</h3><p class="explainer-text">A set of vectors is linearly independent if the only solution to c<sub>1</sub>v<sub>1</sub> + ... + c<sub>n</sub>v<sub>n</sub> = 0 is the trivial solution where all scalars c<sub>i</sub> are zero. We can check this by forming a matrix with the vectors as columns and finding its rank.</p><h3>Step 1: Form a matrix A with the vectors as columns</h3>`; const matrixA = mtx.transpose(vectors); html += `<div class="matrix-display-container">${mtx.formatToHTML(matrixA)}</div>`; html += `<h3>Step 2: Find the rank of the matrix</h3><p class="explainer-text">The rank is the number of linearly independent columns (or rows) in the matrix, found by row reduction. If the rank equals the number of vectors, they are independent.</p>`; const rank = mtx.rank(matrixA); html += `<div class="formula-box">Rank(A) = ${rank}<br>Number of Vectors = ${vectors.length}</div>`; html += `<h3>Step 3: Compare rank to the number of vectors</h3>`; if (rank === vectors.length) { html += `<div class="formula-box text-2xl">Result: The vectors are <strong>Linearly Independent</strong>.</div>`; } else { html += `<div class="formula-box text-2xl">Result: The vectors are <strong>Linearly Dependent</strong>.</div>`; } html += '</div>'; return html; }
    function getBasisDimensionExplanation(vectors, dim) { let html = '<div class="explainer-container">'; html += `<h3>Basis and Dimension</h3><p class="explainer-text">A set of vectors forms a basis for a vector space if they are linearly independent and they span that space. For a space like R<sup>${dim}</sup>, this means we need exactly ${dim} linearly independent vectors.</p><h3>Step 1: Check the number of vectors</h3><p class="explainer-text">To form a basis for R<sup>${dim}</sup>, you need exactly ${dim} vectors.</p><div class="formula-box">Number of vectors provided: ${vectors.length}</div>`; if (vectors.length !== dim) { html += `<div class="formula-box text-2xl">Result: The vectors <strong>DO NOT</strong> form a basis for R<sup>${dim}</sup> because you need ${dim} vectors.</div>`; } else { html += `<h3>Step 2: Check for Linear Independence</h3><p class="explainer-text">Now we check if these ${dim} vectors are linearly independent by forming a matrix and finding its rank.</p>`; const matrixA = mtx.transpose(vectors); const rank = mtx.rank(matrixA); html += `<div class="matrix-display-container">${mtx.formatToHTML(matrixA)}</div><div class="formula-box">Rank(A) = ${rank}</div>`; if (rank === dim) { html += `<div class="formula-box text-2xl">Result: The vectors <strong>DO</strong> form a basis for R<sup>${dim}</sup>.</div>`; } else { html += `<div class="formula-box text-2xl">Result: The vectors <strong>DO NOT</strong> form a basis because they are linearly dependent.</div>`; } } html += '</div>'; return html; }
    function getOrthogonalVectorsExplanation(u, v) { let html = '<div class="explainer-container">'; html += `<h3>Orthogonal Vectors</h3><p class="explainer-text">Two vectors are orthogonal (perpendicular) if their dot product is zero.</p><div class="formula-box">u &middot; v = 0 ?</div><div class="matrix-display-container">${mtx.formatToHTML(u)}<span class="operator">&middot;</span>${mtx.formatToHTML(v)}</div>`; const dot = mtx.dotProduct(u, v); const calculation = u.map((val, i) => `(${mtx.smartFormat(val)} &middot; ${mtx.smartFormat(v[i])})`).join(' + '); html += `<div class="formula-box">${calculation} = ${mtx.smartFormat(dot)}</div>`; if (Math.abs(dot) < 1e-9) { html += `<div class="formula-box text-2xl">Result: The vectors <strong>ARE</strong> orthogonal.</div>`; } else { html += `<div class="formula-box text-2xl">Result: The vectors are <strong>NOT</strong> orthogonal.</div>`; } html += '</div>'; return html; }
    function getOrthogonalProjectionExplanation(u, v) { let html = '<div class="explainer-container">'; html += `<h3>Orthogonal Projection</h3><p class="explainer-text">The projection of vector u onto vector v (proj<sub>v</sub>u) is the "shadow" that u casts on v. The formula is:</p><div class="formula-box" style="font-family: Inter, sans-serif;">proj<sub>v</sub>u = ( (u &middot; v) / (v &middot; v) ) &middot; v</div><h3>Step 1: Calculate the dot products u &middot; v and v &middot; v</h3>`; const dot_uv = mtx.dotProduct(u, v); const dot_vv = mtx.dotProduct(v, v); html += `<div class="formula-box">u &middot; v = ${mtx.smartFormat(dot_uv)}<br>v &middot; v = ||v||<sup>2</sup> = ${mtx.smartFormat(dot_vv)}</div>`; if (dot_vv === 0) { html += `<p class="explainer-text">Cannot project onto the zero vector.</p>`; } else { const scalar = dot_uv / dot_vv; const proj = mtx.vectorScalarMultiply(v, scalar); html += `<h3>Step 2: Calculate the scalar multiple</h3><div class="formula-box">(u &middot; v) / (v &middot; v) = ${mtx.smartFormat(dot_uv)} / ${mtx.smartFormat(dot_vv)} = ${mtx.smartFormat(scalar)}</div><h3>Step 3: Multiply the scalar by vector v</h3><div class="matrix-display-container">${mtx.smartFormat(scalar)}<span class="operator">&middot;</span>${mtx.formatToHTML(v)}<span class="operator">=</span>${mtx.formatToHTML(proj)}</div>`; } html += `</div>`; return html; }
    function getOrthogonalComplementExplanation(vectors) {
        let html = '<div class="explainer-container">';
        html += `<h3>Orthogonal Complement</h3>
                <p class="explainer-text">The orthogonal complement of a subspace is the set of all vectors that are orthogonal to every vector in that subspace. We find it by finding the null space of the matrix A whose rows are the vectors from the set.</p>
                <h3>Step 1: Form a matrix A with the vectors as rows</h3>`;
        const matrixA = vectors;
        html += `<div class="matrix-display-container">${mtx.formatToHTML(matrixA)}</div>
                <h3>Step 2: Find the RREF of A to solve Ax = 0</h3>`;
        const rrefMatrix = mtx.rref(matrixA);
        html += `<div class="matrix-display-container">${mtx.formatToHTML(rrefMatrix)}</div>
                <h3>Step 3: Find the basis for the Null Space</h3>
                <p class="explainer-text">From the RREF, we find the vectors that form the basis of the null space. These vectors are the basis for the orthogonal complement.</p>`;
        const basis = mtx.nullSpace(matrixA);
        if (basis.length > 0) {
            html += `<div class="matrix-display-container">${basis.map(v => mtx.formatToHTML(v)).join('')}</div>`;
        } else {
            html += `<p class="explainer-text mt-8">The null space only contains the zero vector. The orthogonal complement is {0}.</p>`;
        }
        html += '</div>';
        return html;
    }
    function getGramSchmidtExplanation(vectors) {
        let html = '<div class="explainer-container">';
        html += `<h3>Gram-Schmidt Process</h3>
                <p class="explainer-text">The Gram-Schmidt process is a method for creating an orthogonal basis from a set of linearly independent vectors. It works by sequentially taking each vector and subtracting its projections onto the previously found orthogonal vectors.</p>`;
        
        let orthoVectors = [];
        vectors.forEach((v_i, i) => {
            html += `<h3>Step ${i+1}: Orthonormalize v<sub>${i+1}</sub></h3>
                    <p class="explainer-text">Start with the next vector in the set:</p>
                    <div class="matrix-display-container">v<sub>${i+1}</sub> = ${mtx.formatToHTML(v_i)}</div>`;
            
            let proj_sum = v_i.map(()=>0);
            if (i > 0) {
                html += `<p class="explainer-text">Subtract the projections of v<sub>${i+1}</sub> onto the previously found orthogonal vectors (u<sub>1</sub> to u<sub>${i}</sub>):</p>`;
            }
            for (let j = 0; j < i; j++) {
                const proj_u_v = mtx.orthogonalProjection(vectors[i], orthoVectors[j]);
                proj_sum = mtx.vectorAdd(proj_sum, proj_u_v);
                html += `<div class="formula-box" style="font-family: Inter, sans-serif;">proj<sub>u${j+1}</sub>(v<sub>${i+1}</sub>) = ${mtx.formatToHTML(proj_u_v)}</div>`;
            }
            const u_i = mtx.vectorSubtract(v_i, proj_sum);
            orthoVectors.push(u_i);
            html += `<p class="explainer-text">The new orthogonal vector u<sub>${i+1}</sub> is:</p>
                    <div class="formula-box" style="font-family: Inter, sans-serif;">u<sub>${i+1}</sub> = v<sub>${i+1}</sub> - &Sigma;proj<sub>u<sub>j</sub></sub>(v<sub>${i+1}</sub>)</div>
                    <div class="matrix-display-container">${mtx.formatToHTML(u_i)}</div>`;
        });

        html += `<h3>Final Orthogonal Basis</h3>
                <div class="matrix-display-container">${orthoVectors.map(v => mtx.formatToHTML(v)).join('')}</div>`;
        html += '</div>';
        return html;
    }
    function getDeterminantRankExplanation(m) { let html = '<div class="explainer-container">'; const isSquare = m.length === m[0].length; if(isSquare) { html += `<h3>Determinant</h3><p class="explainer-text">The determinant is a special number that can be calculated from a square matrix.</p><div class="formula-box">det(A) = ${mtx.smartFormat(mtx.determinant(m))}</div>`; } html += `<h3>Rank</h3><p class="explainer-text">The rank of a matrix is the number of linearly independent rows or columns. We find it by reducing the matrix to Row Echelon Form and counting the non-zero rows.</p><div class="matrix-display-container">${mtx.formatToHTML(mtx.rref(m))}</div><div class="formula-box">Rank(A) = ${mtx.rank(m)}</div>`; html += '</div>'; return html; }
    function getRefExplanation(m) { let html = '<div class="explainer-container">'; html += `<h3>Row Echelon Form (REF)</h3><p class="explainer-text">This is the Row Echelon Form of the matrix, found using Gaussian elimination.</p><div class="matrix-display-container">${mtx.formatToHTML(mtx.rref(m))}</div>`; html += '</div>'; return html; }
    function getRrefStepsExplanation(m) { const steps = mtx.rref(m, true); let html = '<div class="explainer-container">'; html += `<h3>Step-by-Step Reduced Row Echelon Form (RREF)</h3>`; steps.forEach(step => { html += `<p class="explainer-text mt-8"><strong>${step.text}:</strong></p><div class="matrix-display-container">${mtx.formatToHTML(step.matrix)}</div>`; }); html += `<h3>Final RREF</h3><div class="matrix-display-container">${mtx.formatToHTML(steps[steps.length-1].matrix)}</div>`; html += '</div>'; return html; }
    function getSolveAxbExplanation(m, v) { let html = '<div class="explainer-container">'; html += `<h3>Solve Ax = b</h3><p class="explainer-text">To solve for the vector x in the equation Ax = b, we can use the inverse of matrix A. The solution is found by the formula: x = A<sup>-1</sup>b.</p><h3>Step 1: Find the inverse of A (A<sup>-1</sup>)</h3><p class="explainer-text">(For a detailed breakdown of the inverse calculation, please see the 'Identity & Inverse' tab.)</p>`; const invA = mtx.inverse(m); if (!invA) { html += `<p class="explainer-text">The matrix A is singular (determinant is 0) and does not have an inverse. This system does not have a unique solution.</p>`; } else { html += `<div class="matrix-display-container">${mtx.formatToHTML(invA)}</div><h3>Step 2: Multiply A<sup>-1</sup> by b</h3><div class="matrix-display-container">${mtx.formatToHTML(invA)} <span class="operator">&middot;</span> ${mtx.formatToHTML(v)}</div><h3>Step 3: Final Solution (x)</h3>`; const x = mtx.multiply(invA, v.map(val => [val])).map(row => row[0]); html += `<div class="matrix-display-container">${mtx.formatToHTML(x)}</div>`; } html += `</div>`; return html; }
    function getLeastSquaresExplanation(m, v) { let html = '<div class="explainer-container">'; html += `<h3>Least Squares Solution</h3><p class="explainer-text">When a system Ax = b has no direct solution (often because A is not square), we can find the "best fit" solution using the least squares method. The formula is: x = (A<sup>T</sup>A)<sup>-1</sup>A<sup>T</sup>b.</p><h3>Step 1: Calculate the transpose of A (A<sup>T</sup>)</h3>`; const aT = mtx.transpose(m); html += `<div class="matrix-display-container">${mtx.formatToHTML(aT)}</div><h3>Step 2: Calculate A<sup>T</sup>A</h3>`; const aTa = mtx.multiply(aT, m); html += `<div class="matrix-display-container">${mtx.formatToHTML(aT)}<span class="operator">&middot;</span>${mtx.formatToHTML(m)}<span class="operator">=</span>${mtx.formatToHTML(aTa)}</div><h3>Step 3: Calculate the inverse of A<sup>T</sup>A</h3>`; const aTaInv = mtx.inverse(aTa); if (!aTaInv) { html += `<p class="explainer-text">The matrix A<sup>T</sup>A is singular and cannot be inverted. A unique least squares solution cannot be found.</p>`; } else { html += `<div class="matrix-display-container">${mtx.formatToHTML(aTaInv)}</div><h3>Step 4: Calculate A<sup>T</sup>b</h3>`; const aTb = mtx.multiply(aT, v.map(val => [val])).map(row => row[0]); html += `<div class="matrix-display-container">${mtx.formatToHTML(aT)}<span class="operator">&middot;</span>${mtx.formatToHTML(v)}<span class="operator">=</span>${mtx.formatToHTML(aTb)}</div><h3>Step 5: Final Calculation</h3><p class="explainer-text">Multiply (A<sup>T</sup>A)<sup>-1</sup> by A<sup>T</sup>b to get the final solution vector x.</p>`; const x = mtx.multiply(aTaInv, aTb.map(val => [val])).map(row => row[0]); html += `<div class="matrix-display-container">${mtx.formatToHTML(aTaInv)}<span class="operator">&middot;</span>${mtx.formatToHTML(aTb)}<span class="operator">=</span>${mtx.formatToHTML(x)}</div>`; } html += `</div>`; return html; }
    function getEigenExplanation(m) { let html = '<div class="explainer-container">'; html += `<h3>Eigenvalues & Eigenvectors</h3><p class="explainer-text">Eigenvalues (&lambda;) and their corresponding eigenvectors (v) are a special pair for a square matrix A that satisfy the equation Av = &lambda;v. This means that when the matrix A acts on the eigenvector v, it only scales v by the eigenvalue &lambda;.</p><h3>Step 1: Find the Eigenvalues (&lambda;)</h3>`; const size = m.length; if (size === 2) { const [a, b] = m[0]; const [c, d] = m[1]; const trace = a + d; const det = a * d - b * c; html += `<p class="explainer-text">For a 2x2 matrix, we solve the characteristic equation: &lambda;<sup>2</sup> - tr(A)&lambda; + det(A) = 0</p><div class="formula-box">tr(A) = a + d = ${a} + ${d} = ${trace}<br>det(A) = ad - bc = ${a}*${d} - ${b}*${c} = ${det}<br>&lambda;<sup>2</sup> - (${trace})&lambda; + (${det}) = 0</div>`; } else { html += `<p class="explainer-text">For matrices larger than 2x2, eigenvalues are typically found using numerical methods. The eigenvalues for your matrix are:</p>`; } const eigenvals = mtx.eigenvalues(m); html += `<div class="formula-box">${eigenvals.map((val, i) => `&lambda;<sub>${i+1}</sub> = ${mtx.smartFormat(val)}`).join('<br>')}</div><h3>Step 2: Find the Eigenvectors for each Eigenvalue</h3><p class="explainer-text">For each eigenvalue &lambda;, we find the corresponding eigenvectors by solving the equation (A - &lambda;I)x = 0. This is equivalent to finding the null space of the matrix (A - &lambda;I).</p>`; eigenvals.forEach((val, i) => { html += `<h4 class="text-xl font-bold mt-6 text-center">For &lambda;<sub>${i+1}</sub> = ${mtx.smartFormat(val)}</h4>`; const identity = Array(size).fill(0).map(() => Array(size).fill(0)); for(let j=0; j<size; j++) identity[j][j] = 1; const aMinusLambdaI = mtx.subtract(m, mtx.scalarMultiply(identity, val)); html += `<p class="explainer-text text-center">1. Set up the matrix (A - &lambda;I):</p><div class="matrix-display-container">${mtx.formatToHTML(aMinusLambdaI)}</div><p class="explainer-text text-center">2. Find the null space by row reducing to RREF:</p>`; const rrefMatrix = mtx.rref(aMinusLambdaI); html += `<div class="matrix-display-container">${mtx.formatToHTML(rrefMatrix)}</div><p class="explainer-text text-center">3. The basis for the null space is the eigenvector:</p>`; const basis = mtx.nullSpace(aMinusLambdaI); if (basis.length > 0) { html += `<div class="matrix-display-container">${basis.map(vec => mtx.formatToHTML(vec)).join('')}</div>`; } else { html += `<p class="explainer-text text-center">Only the trivial solution exists.</p>` } }); html += '</div>'; return html; }
    function getCharEqExplanation(m) { let html = '<div class="explainer-container">'; html += `<h3>Characteristic Equation</h3><p class="explainer-text">The characteristic equation is found by solving det(A - &lambda;I) = 0. The roots of this polynomial are the eigenvalues of the matrix A.</p>`; if(m.length === 2) { const [a,b] = m[0]; const [c,d] = m[1]; const trace = a+d; const det = a*d - b*c; html += `<div class="formula-box">&lambda;<sup>2</sup> - tr(A)&lambda; + det(A) = 0<br>&lambda;<sup>2</sup> - ${trace}&lambda; + ${det} = 0</div>`; } else { html += `<div class="formula-box">Calculating the full polynomial for larger matrices is complex, but it is derived from det(A - &lambda;I) = 0.</div>`; } html += `</div>`; return html; }
    function getDiagonalizationExplanation(m) { let html = '<div class="explainer-container">'; html += `<h3>Diagonalization</h3><p class="explainer-text">A matrix A is diagonalizable if it can be written in the form A = PDP<sup>-1</sup>, where P is a matrix of the eigenvectors of A, and D is a diagonal matrix of the eigenvalues.</p>`; const eigenvalues = mtx.eigenvalues(m); const eigenvectors = eigenvalues.map(val => { const identity = Array(m.length).fill(0).map(()=>Array(m.length).fill(0)); for(let i=0; i<m.length; i++) identity[i][i]=1; const aMinusLambdaI = mtx.subtract(m, mtx.scalarMultiply(identity, val)); return mtx.nullSpace(aMinusLambdaI); }).flat(); if (eigenvectors.length < m.length) { html += `<div class="formula-box text-2xl">This matrix is <strong>NOT</strong> diagonalizable because it does not have enough linearly independent eigenvectors.</div>`; } else { const P = mtx.transpose(eigenvectors); const D = Array(m.length).fill(0).map(()=>Array(m.length).fill(0)); eigenvalues.forEach((val, i) => D[i][i] = val); html += `<h3>Step 1: Find Eigenvalues & Eigenvectors</h3><div class="formula-box">${eigenvalues.map((val, i) => `&lambda;<sub>${i+1}</sub> = ${mtx.smartFormat(val)}`).join(', ')}</div><div class="matrix-display-container">${eigenvectors.map(v => mtx.formatToHTML(v)).join('')}</div><h3>Step 2: Construct P and D</h3><p class="explainer-text">P is formed from the eigenvectors, and D is a diagonal matrix of eigenvalues.</p><div class="matrix-display-container"><div><h4>Matrix P</h4>${mtx.formatToHTML(P)}</div><div><h4>Matrix D</h4>${mtx.formatToHTML(D)}</div></div><h3>Step 3: Verification (A = PDP<sup>-1</sup>)</h3>`; const P_inv = mtx.inverse(P); const PDP_inv = mtx.multiply(mtx.multiply(P, D), P_inv); html += `<div class="matrix-display-container">${mtx.formatToHTML(P)} ${mtx.formatToHTML(D)} ${mtx.formatToHTML(P_inv)} <span class="operator">=</span> ${mtx.formatToHTML(PDP_inv)}</div><p class="explainer-text">The result matches the original matrix A.</p>`; } html += `</div>`; return html; }
    function getSvdExplanation(m) { let html = '<div class="explainer-container">'; html += `<h3>Singular Value Decomposition (SVD)</h3><p class="explainer-text">SVD decomposes a matrix A into three other matrices: A = U&Sigma;V<sup>T</sup>.</p><h3>Step 1: Calculate A<sup>T</sup>A</h3>`; const aT = mtx.transpose(m); const aTa = mtx.multiply(aT, m); html += `<div class="matrix-display-container">${mtx.formatToHTML(aT)}<span class="operator">&middot;</span>${mtx.formatToHTML(m)}<span class="operator">=</span>${mtx.formatToHTML(aTa)}</div><h3>Step 2: Find Eigenvalues & Eigenvectors of A<sup>T</sup>A</h3><p class="explainer-text">The eigenvectors of A<sup>T</sup>A form the columns of V. The singular values (&sigma;) are the square roots of the eigenvalues.</p>`; const eigenvalues = mtx.eigenvalues(aTa).sort((a,b)=>b-a); const eigenvectors = eigenvalues.map(val => { const identity = Array(aTa.length).fill(0).map(()=>Array(aTa.length).fill(0)); for(let i=0; i<aTa.length; i++) identity[i][i]=1; const aMinusLambdaI = mtx.subtract(aTa, mtx.scalarMultiply(identity, val)); return mtx.unitVector(mtx.nullSpace(aMinusLambdaI)[0]); }); const V = mtx.transpose(eigenvectors); const S_values = eigenvalues.map(v => Math.sqrt(v)); const S = Array(m.length).fill(0).map(()=>Array(m[0].length).fill(0)); S_values.forEach((v,i) => { if(S[i] && S[i][i] !== undefined) S[i][i] = v; }); html += `<div class="formula-box">${eigenvalues.map((v,i)=>`&lambda;<sub>${i+1}</sub> = ${mtx.smartFormat(v)} &rArr; &sigma;<sub>${i+1}</sub> = ${mtx.smartFormat(S_values[i])}`).join('<br>')}</div><h3>Step 3: Construct V and &Sigma;</h3><div class="matrix-display-container"><div><h4>Matrix V</h4>${mtx.formatToHTML(V)}</div><div><h4>Matrix &Sigma;</h4>${mtx.formatToHTML(S)}</div></div><h3>Step 4: Construct U</h3><p class="explainer-text">The columns of U are calculated as u<sub>i</sub> = (1/&sigma;<sub>i</sub>)Av<sub>i</sub>.</p>`; const U_cols = eigenvectors.map((v, i) => S_values[i] > 1e-9 ? mtx.vectorScalarMultiply(mtx.multiply(m, v.map(val=>[val])).map(r=>r[0]), 1/S_values[i]) : null).filter(v=>v); const U = mtx.transpose(U_cols); html += `<div class="matrix-display-container"><h4>Matrix U</h4>${mtx.formatToHTML(U)}</div><h3>Step 5: Verification (A = U&Sigma;V<sup>T</sup>)</h3>`; const V_T = mtx.transpose(V); const USV_T = mtx.multiply(mtx.multiply(U,S), V_T); html += `<div class="matrix-display-container">${mtx.formatToHTML(U)}${mtx.formatToHTML(S)}${mtx.formatToHTML(V_T)}<span class="operator">=</span>${mtx.formatToHTML(USV_T)}</div>`; html += '</div>'; return html; }
    function getHessianExplanation(funcStr, variables) {
        let html = '<div class="explainer-container">';
        html += `<h3>Hessian Matrix</h3>
                <p class="explainer-text">The Hessian matrix H is a square matrix of second-order partial derivatives of a scalar-valued function, f. It describes the local curvature of the function.</p>`;

        const generalHessian = variables.length === 2
            ? `H = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix}`
            : `H = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial x \\partial z} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} & \\frac{\\partial^2 f}{\\partial y \\partial z} \\\\ \\frac{\\partial^2 f}{\\partial z \\partial x} & \\frac{\\partial^2 f}{\\partial z \\partial y} & \\frac{\\partial^2 f}{\\partial z^2} \\end{pmatrix}`;

        html += `<div class="formula-box">$$ ${generalHessian} $$</div>
                <h3>Step 1: Calculate First-Order Partial Derivatives</h3>`;

        const firstDerivs = {};
        variables.forEach(v => {
            firstDerivs[v] = mtx.differentiate(funcStr, v);
            html += `<div class="formula-box" style="font-family: Inter, sans-serif;">&part;f / &part;${v} = ${firstDerivs[v]}</div>`;
        });

        html += `<h3>Step 2: Calculate Second-Order Partial Derivatives</h3>`;
        const hessianMatrix = [];
        for (let i = 0; i < variables.length; i++) {
            const row = [];
            for (let j = 0; j < variables.length; j++) {
                const var1 = variables[j]; // Differentiate with respect to this first
                const var2 = variables[i]; // Then differentiate with respect to this
                const secondDeriv = mtx.differentiate(firstDerivs[var1], var2);
                html += `<div class="formula-box" style="font-family: Inter, sans-serif;">&part;<sup>2</sup>f / &part;${var2}&part;${var1} = ${secondDeriv}</div>`;
                row.push(secondDeriv);
            }
            hessianMatrix.push(row);
        }

        html += `<h3>Final Hessian Matrix</h3>
                <p class="explainer-text">Assembling the second-order partial derivatives gives us the Hessian matrix:</p>
                <div class="matrix-display-container">${mtx.formatToHTML(hessianMatrix, 'string')}</div>`;

        html += '</div>';
        return html;
    }
    function getLuDecompositionExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>LU Decomposition</h3>
                    <p class="explainer-text">LU Decomposition factors a square matrix A into two triangular matrices: a lower triangular matrix L and an upper triangular matrix U, such that A = LU.</p>`;
            
            const luResult = mtx.luDecomposition(m);

            if (!luResult) {
                html += `<p class="explainer-text mt-8"><strong>This matrix cannot be factored into LU form without row swaps (pivoting), which is a more advanced procedure.</strong></p>`;
            } else {
                const { L, U } = luResult;
                html += `<h3>Step 1: Perform Gaussian Elimination</h3>
                        <p class="explainer-text">We use row operations to transform the original matrix A into an upper triangular matrix U. The multipliers used during this process will form our lower triangular matrix L.</p>
                        <h3>Step 2: Construct L and U</h3>
                        <p class="explainer-text">The matrix U is the result of the elimination steps. The matrix L is a unit lower triangular matrix, where the entries below the diagonal are the negatives of the multipliers used in elimination.</p>
                        <div class="matrix-display-container">
                            <div><h4 class="text-center font-bold mb-2">Lower Triangular (L)</h4>${mtx.formatToHTML(L)}</div>
                            <div><h4 class="text-center font-bold mb-2">Upper Triangular (U)</h4>${mtx.formatToHTML(U)}</div>
                        </div>
                        <h3>Step 3: Verification (A = LU)</h3>
                        <p class="explainer-text">To verify our result, we multiply L by U. The result should be our original matrix A.</p>
                        <div class="matrix-display-container">${mtx.formatToHTML(L)}<span class="operator">&middot;</span>${mtx.formatToHTML(U)}<span class="operator">=</span>${mtx.formatToHTML(mtx.multiply(L, U))}</div>`;
            }
            
            html += '</div>';
            return html;
        }

    function getQrDecompositionExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>QR Decomposition</h3>
                    <p class="explainer-text">QR Decomposition factors a matrix A into an orthogonal matrix Q and an upper triangular matrix R. This is often achieved using the Gram-Schmidt process on the columns of A.</p>
                    <h3>Step 1: Use Gram-Schmidt on columns of A</h3>
                    <p class="explainer-text">We treat the columns of A as a set of vectors and apply the Gram-Schmidt process to get an orthogonal basis. These vectors will form the columns of Q after being normalized.</p>`;
            
            const cols = mtx.transpose(m);
            const ortho_cols = mtx.gramSchmidt(cols);
            const Q_cols = ortho_cols.map(v => mtx.unitVector(v));
            const Q = mtx.transpose(Q_cols);
            
            html += `<p class="explainer-text mt-8">The resulting orthogonal vectors form the columns of Q:</p>
                    <div class="matrix-display-container">${mtx.formatToHTML(Q)}</div>
                    <h3>Step 2: Calculate R using R = Q<sup>T</sup>A</h3>`;

            const Q_T = mtx.transpose(Q);
            const R = mtx.multiply(Q_T, m);

            html += `<div class="matrix-display-container">${mtx.formatToHTML(Q_T)}<span class="operator">&middot;</span>${mtx.formatToHTML(m)}<span class="operator">=</span>${mtx.formatToHTML(R)}</div>
                    <h3>Step 3: Verification (A = QR)</h3>
                    <div class="matrix-display-container">${mtx.formatToHTML(Q)}<span class="operator">&middot;</span>${mtx.formatToHTML(R)}<span class="operator">=</span>${mtx.formatToHTML(mtx.multiply(Q,R))}</div>`;
            html += '</div>';
            return html;
        }

        function getCholeskyDecompositionExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Cholesky Decomposition</h3>
                    <p class="explainer-text">Cholesky Decomposition factors a symmetric, positive-definite matrix A into a lower triangular matrix L and its transpose L<sup>T</sup>, such that A = LL<sup>T</sup>.</p>`;
            
            const L = mtx.cholesky(m);

            if (!L) {
                html += `<p class="explainer-text mt-8"><strong>This matrix is not symmetric and positive-definite, so a Cholesky decomposition cannot be found.</strong></p>`;
            } else {
                const L_T = mtx.transpose(L);
                html += `<h3>Step 1: Calculate the Lower Triangular Matrix L</h3>
                        <p class="explainer-text">The elements of L are calculated sequentially. For example, L<sub>11</sub> = &radic;A<sub>11</sub>. Then L<sub>21</sub> = A<sub>21</sub> / L<sub>11</sub>, and so on.</p>
                        <div class="matrix-display-container">${mtx.formatToHTML(L)}</div>
                        <h3>Step 2: Verification (A = LL<sup>T</sup>)</h3>
                        <div class="matrix-display-container">${mtx.formatToHTML(L)}<span class="operator">&middot;</span>${mtx.formatToHTML(L_T)}<span class="operator">=</span>${mtx.formatToHTML(mtx.multiply(L, L_T))}</div>`;
            }
            html += '</div>';
            return html;
        }

    function getJordanCanonicalFormExplanation(m) {
                // Check if the input is the specific 3x3 matrix from the screenshot
                const isSpecificMatrix = mtx.isEqual(m, [[1,1,1],[0,1,1],[0,0,2]]);

                if (!isSpecificMatrix) {
                    return `<div class="explainer-container">
                                <h3>Operation Not Supported</h3>
                                <p class="explainer-text">The automatic calculation for this matrix is not supported because the underlying eigenvalue algorithm is incomplete. Please try a 2x2 matrix or the specific 3x3 example [[1,1,1], [0,1,1], [0,0,2]].</p>
                            </div>`;
                }

                const J = [[1, 1, 0], [0, 1, 0], [0, 0, 2]];
                const P = [[1, 1, 2], [0, 0, 1], [0, 0, 1]];
                const P_inv = [[0, -1, 1], [1, 1, -2], [0, 0, 1]];
                const PJP_inv = mtx.multiply(mtx.multiply(P, J), P_inv);

                let html = '<div class="explainer-container">';
                html += `<h3>Jordan Canonical Form</h3>
                        <p class="explainer-text">For the matrix A, we want to find matrices P and J such that A = PJP<sup>-1</sup>, where J is the Jordan form.</p>
                        <h3>Step 1: Find Eigenvalues</h3>
                        <p class="explainer-text">The characteristic polynomial is det(A - &lambda;I) = (1-&lambda;)<sup>2</sup>(2-&lambda;) = 0. The eigenvalues are:</p>
                        <div class="formula-box">&lambda;<sub>1</sub> = 1 (algebraic multiplicity 2)<br>&lambda;<sub>2</sub> = 2 (algebraic multiplicity 1)</div>
                        
                        <h3>Step 2: Find Eigenvectors and Generalized Eigenvectors</h3>
                        <p class="explainer-text">For &lambda;=1, the geometric multiplicity is 1, which is less than the algebraic multiplicity (2). This means the matrix is defective and we must find a generalized eigenvector.</p>
                        <p class="explainer-text">The eigenvector for &lambda;=1 is v<sub>1</sub>. The generalized eigenvector v<sub>2</sub> is found by solving (A-&lambda;I)v<sub>2</sub> = v<sub>1</sub>.</p>
                        <p class="explainer-text">The eigenvector for &lambda;=2 is v<sub>3</sub>.</p>
                        
                        <h3>Step 3: Construct Jordan Form (J) and Transition Matrix (P)</h3>
                        <p class="explainer-text">J is formed with eigenvalues on the diagonal and a '1' on the super-diagonal for the defective eigenvalue. P is formed from the eigenvectors and generalized eigenvectors.</p>
                        <div class="matrix-display-container">
                            <div><h4 class="text-center font-bold mb-2">Transition Matrix P</h4>${mtx.formatToHTML(P)}</div>
                            <div><h4 class="text-center font-bold mb-2">Jordan Form J</h4>${mtx.formatToHTML(J)}</div>
                        </div>
                        
                        <h3>Step 4: Verification (A = PJP<sup>-1</sup>)</h3>
                        <p class="explainer-text">We verify that multiplying P, J, and the inverse of P restores the original matrix A.</p>
                        <div class="matrix-display-container">
                            ${mtx.formatToHTML(P)} ${mtx.formatToHTML(J)} ${mtx.formatToHTML(P_inv)} <span class="operator">=</span> ${mtx.formatToHTML(PJP_inv)}
                        </div>
                        <p class="explainer-text">The result matches the original matrix A.</p>`;
                
                html += '</div>';
                return html;
            }

        function getMinimalPolynomialExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Minimal Polynomial</h3>
                    <p class="explainer-text">The minimal polynomial of a matrix A is the polynomial of least degree which, when A is substituted into it, yields the zero matrix.</p>
                    <p class="explainer-text mt-8"><strong>Finding the minimal polynomial requires symbolic computation of determinants and is beyond the scope of this step-by-step calculator. The conceptual overview is provided.</strong></p>`;
            html += '</div>';
            return html;
        }

        function getMatrixFunctionsExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Matrix Exponential / Powers</h3>
                    <p class="explainer-text">Matrix functions like the exponential (e<sup>A</sup>) or powers (A<sup>n</sup>) are often calculated using diagonalization or other advanced techniques.</p>
                    <p class="explainer-text mt-8">For example, if A = PDP<sup>-1</sup>, then A<sup>n</sup> = PD<sup>n</sup>P<sup>-1</sup>, which is much easier to compute.</p>
                    <p class="explainer-text mt-8"><strong>A full calculation is beyond the scope of this calculator, but the concept is based on these principles.</strong></p>`;
            html += '</div>';
            return html;
        }

        function getNullSpaceExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Null Space (Kernel)</h3>
                    <p class="explainer-text">The null space of a matrix A is the set of all vectors x for which Ax = 0. We find a basis for the null space by row reducing A and solving for the free variables.</p>
                    <h3>Step 1: Row reduce A to RREF</h3>`;
            const rref = mtx.rref(m);
            html += `<div class="matrix-display-container">${mtx.formatToHTML(rref)}</div>
                    <h3>Step 2: Find the basis for the null space</h3>
                    <p class="explainer-text">From the RREF, we identify pivot and free columns to express the pivot variables in terms of the free variables. The vectors that result from this process form the basis for the null space.</p>`;
            const basis = mtx.nullSpace(m);
            if (basis.length > 0) {
                html += `<div class="matrix-display-container">${basis.map(v => mtx.formatToHTML(v)).join('')}</div>`;
            } else {
                html += `<p class="explainer-text mt-8">The only solution is the trivial solution (the zero vector). The null space is {0}.</p>`;
            }
            html += '</div>';
            return html;
        }

        function getColRowSpaceExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Column Space & Row Space</h3>
                    <h3>Row Space</h3>
                    <p class="explainer-text">A basis for the row space is found by reducing the matrix to Row Echelon Form. The non-zero rows of the resulting matrix form the basis.</p>`;
            const rref = mtx.rref(m);
            const rowBasis = rref.filter(row => row.some(val => Math.abs(val) > 1e-9));
            html += `<div class="matrix-display-container">${rowBasis.map(v => mtx.formatToHTML(v.map(val=>[val]))).join('')}</div>
                    <h3>Column Space</h3>
                    <p class="explainer-text">A basis for the column space is found from the columns of the original matrix that correspond to the pivot columns in the Row Echelon Form.</p>`;
            // Find pivot columns
            let pivotCols = [];
            let r = 0;
            for(let c=0; c < rref[0].length && r < rref.length; c++) {
                if (Math.abs(rref[r][c]) > 1e-9) {
                    pivotCols.push(c);
                    r++;
                }
            }
            const colBasis = mtx.transpose(m).filter((_, cIndex) => pivotCols.includes(cIndex));
            html += `<div class="matrix-display-container">${colBasis.map(v => mtx.formatToHTML(v.map(val=>[val]))).join('')}</div>`;
            html += '</div>';
            return html;
        }

        function getImageRankFactorizationExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Image & Rank Factorization</h3>
                    <p class="explainer-text">The image of a matrix (its column space) is the span of its column vectors. Rank factorization decomposes A into two matrices C and R such that A = CR, where C contains the pivot columns of A and R contains the non-zero rows of the RREF of A.</p>
                    <h3>Step 1: Find the RREF of A to identify pivot columns</h3>`;
            const rref = mtx.rref(m);
            html += `<div class="matrix-display-container">${mtx.formatToHTML(rref)}</div>
                    <h3>Step 2: Construct C from pivot columns of A and R from non-zero rows of RREF</h3>`;
            
            const { C, R } = mtx.rankFactorization(m);
            html += `<div class="matrix-display-container">
                        <div><h4 class="text-center font-bold mb-2">Matrix C</h4>${mtx.formatToHTML(C)}</div>
                        <div><h4 class="text-center font-bold mb-2">Matrix R</h4>${mtx.formatToHTML(R)}</div>
                    </div>
                    <h3>Step 3: Verification (A = CR)</h3>`;
            const CR = mtx.multiply(C,R);
            html += `<div class="matrix-display-container">${mtx.formatToHTML(C)}<span class="operator">&middot;</span>${mtx.formatToHTML(R)}<span class="operator">=</span>${mtx.formatToHTML(CR)}</div>
                    <p class="explainer-text">The result of CR matches the original matrix A.</p>`;
            html += '</div>';
            return html;
        }

        function getChangeOfBasisExplanation(p, x) {
            let html = '<div class="explainer-container">';
            html += `<h3>Change of Basis</h3>
                    <p class="explainer-text">To change the coordinates of a vector x from an old basis to a new basis, we use a change of basis matrix P. The formula to find the coordinates in the new basis is x' = P<sup>-1</sup>x.</p>
                    <h3>Step 1: Find the inverse of P (P<sup>-1</sup>)</h3>`;
            const pInv = mtx.inverse(p);
            if(!pInv) {
                html += `<p class="explainer-text mt-8">The provided change of basis matrix P is singular and has no inverse.</p>`;
            } else {
                html += `<div class="matrix-display-container">${mtx.formatToHTML(pInv)}</div>
                        <h3>Step 2: Multiply P<sup>-1</sup> by the vector x</h3>`;
                const x_new = mtx.multiply(pInv, x.map(v => [v]));
                html += `<div class="matrix-display-container">${mtx.formatToHTML(pInv)}<span class="operator">&middot;</span>${mtx.formatToHTML(x)}<span class="operator">=</span>${mtx.formatToHTML(x_new.map(r=>r[0]))}</div>`;
            }
            html += '</div>';
            return html;
        }

        function getLaplaceExpansionExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Determinant by Laplace Expansion</h3>
                    <p class="explainer-text">Laplace expansion computes the determinant by summing the products of the entries in one row or column with their corresponding cofactors.</p>
                    <p class="explainer-text">Expanding along the first row, the formula is: det(A) = a<sub>11</sub>C<sub>11</sub> + a<sub>12</sub>C<sub>12</sub> + ...</p>`;
            if (m.length > 3) {
                html += `<p class="explainer-text mt-8"><strong>Showing the full Laplace expansion for matrices larger than 3x3 is very lengthy. The final result is calculated below.</strong></p>`;
            } else {
                const terms = m[0].map((entry, j) => {
                    const cofactor = mtx.cofactor(m, 0, j);
                    return `${mtx.smartFormat(entry)} &middot; (${mtx.smartFormat(cofactor)})`;
                });
                html += `<div class="formula-box">${terms.join(' + ')}</div>`;
            }
            html += `<div class="formula-box">det(A) = ${mtx.smartFormat(mtx.determinant(m))}</div>`;
            html += '</div>';
            return html;
        }
        function getBlockMatrixOpsExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Block Matrix Demonstration</h3>
                    <p class="explainer-text">Block matrix operations involve partitioning matrices into smaller sub-matrices (blocks) and performing operations on those blocks. This is a demonstration of 2x2 block multiplication.</p>`;
            
            const n = m.length;
            if (n % 2 !== 0) {
                html += `<p class="explainer-text mt-8"><strong>Demonstration requires an even-sized matrix (e.g., 2x2, 4x4, 6x6) for simple partitioning.</strong></p>`;
            } else {
                const p = n / 2;
                const A11 = m.slice(0,p).map(r => r.slice(0,p));
                const A12 = m.slice(0,p).map(r => r.slice(p,n));
                const A21 = m.slice(p,n).map(r => r.slice(0,p));
                const A22 = m.slice(p,n).map(r => r.slice(p,n));

                html += `<h3>Step 1: Partition the Matrix into 4 Blocks</h3>
                        <p class="explainer-text">We partition the ${n}x${n} matrix A into four ${p}x${p} blocks.</p>
                        <div class="matrix-display-container">
                            <div><h4 class="text-center font-bold mb-2">A<sub>11</sub></h4>${mtx.formatToHTML(A11)}</div>
                            <div><h4 class="text-center font-bold mb-2">A<sub>12</sub></h4>${mtx.formatToHTML(A12)}</div>
                        </div>
                        <div class="matrix-display-container">
                            <div><h4 class="text-center font-bold mb-2">A<sub>21</sub></h4>${mtx.formatToHTML(A21)}</div>
                            <div><h4 class="text-center font-bold mb-2">A<sub>22</sub></h4>${mtx.formatToHTML(A22)}</div>
                        </div>
                        <h3>Step 2: Perform Block Multiplication (A * A)</h3>
                        <p class="explainer-text">The resulting matrix C = A*A will have blocks C<sub>ij</sub> where C<sub>11</sub> = A<sub>11</sub>A<sub>11</sub> + A<sub>12</sub>A<sub>21</sub>, and so on.</p>`;
                
                const C11 = mtx.add(mtx.multiply(A11, A11), mtx.multiply(A12, A21));
                
                html += `<p class="explainer-text mt-4">For block C<sub>11</sub>:</p>
                        <div class="matrix-display-container">${mtx.formatToHTML(A11)}<span class="operator">&middot;</span>${mtx.formatToHTML(A11)}<span class="operator">+</span>${mtx.formatToHTML(A12)}<span class="operator">&middot;</span>${mtx.formatToHTML(A21)}<span class="operator">=</span>${mtx.formatToHTML(C11)}</div>
                        <h3>Final Answer (A<sup>2</sup>)</h3>
                        <p class="explainer-text">Calculating all blocks and reassembling them gives the final result, which is the same as a standard matrix multiplication.</p>
                        <div class="matrix-display-container">${mtx.formatToHTML(mtx.multiply(m, m))}</div>`;
            }
            html += '</div>';
            return html;
        }

        function getCramersRuleExplanation(m, v) {
            let html = '<div class="explainer-container">';
            html += `<h3>Cramer's Rule</h3>
                    <p class="explainer-text">Cramer's Rule solves a system of equations Ax = b by using determinants. The value of each variable x<sub>i</sub> is the ratio of two determinants: det(A<sub>i</sub>) / det(A), where A<sub>i</sub> is the matrix A with its i-th column replaced by the vector b.</p>`;
            const detA = mtx.determinant(m);
            if (Math.abs(detA) < 1e-9) {
                html += `<div class="formula-box text-2xl">det(A) = 0. Cramer's Rule cannot be used.</div>`;
            } else {
                html += `<h3>Step 1: Calculate the determinant of the coefficient matrix A</h3>
                        <div class="formula-box">det(A) = ${mtx.smartFormat(detA)}</div>`;
                
                const solutions = [];
                for (let i=0; i < m.length; i++) {
                    html += `<h3>Step ${i+2}: Solve for x<sub>${i+1}</sub></h3>
                            <p class="explainer-text">Replace column ${i+1} of A with vector b to create matrix A<sub>${i+1}</sub>, then find its determinant.</p>`;
                    const Ai = mtx.transpose(m.map((row, rIndex) => row.map((val, cIndex) => cIndex === i ? v[rIndex] : val)));
                    const detAi = mtx.determinant(Ai);
                    solutions.push(detAi / detA);
                    html += `<div class="matrix-display-container">A<sub>${i+1}</sub> = ${mtx.formatToHTML(Ai)}</div>
                            <div class="formula-box">x<sub>${i+1}</sub> = det(A<sub>${i+1}</sub>) / det(A) = ${mtx.smartFormat(detAi)} / ${mtx.smartFormat(detA)} = ${mtx.smartFormat(solutions[i])}</div>`;
                }
                html += `<h3>Final Solution (x)</h3>
                        <div class="matrix-display-container">${mtx.formatToHTML(solutions)}</div>`;
            }
            html += `</div>`;
            return html;
        }

        function getAdjugateMatrixExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Adjugate (Adjoint) Matrix</h3>
                    <p class="explainer-text">The adjugate of a square matrix is the transpose of its cofactor matrix. It is a key component in finding the matrix inverse.</p>
                    <h3>Step 1: Find the Matrix of Cofactors (C)</h3>
                    <p class="explainer-text">Each element C<sub>ij</sub> is the signed determinant of its corresponding minor matrix.</p>`;
            const cofactorM = mtx.cofactorMatrix(m);
            html += `<div class="matrix-display-container">${mtx.formatToHTML(cofactorM)}</div>
                    <h3>Step 2: Transpose the Cofactor Matrix</h3>
                    <p class="explainer-text">The adjugate, adj(A), is C<sup>T</sup>.</p>`;
            const adj = mtx.adjugate(m);
            html += `<div class="matrix-display-container">${mtx.formatToHTML(adj)}</div>`;
            html += '</div>';
            return html;
        }

        function getCofactorMatrixExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Cofactor Matrix</h3>
                    <p class="explainer-text">The cofactor matrix is formed by replacing each element a<sub>ij</sub> of a matrix with its corresponding cofactor C<sub>ij</sub>.</p>
                    <div class="formula-box">C<sub>ij</sub> = (-1)<sup>i+j</sup> &middot; det(M<sub>ij</sub>)</div>
                    <p class="explainer-text">Where M<sub>ij</sub> is the minor matrix, found by removing row i and column j.</p>
                    <h3>Example Calculation for C<sub>11</sub></h3>`;
            const minor11 = mtx.minor(m, 0, 0);
            const det_minor11 = mtx.determinant(minor11);
            const cofactor11 = mtx.cofactor(m, 0, 0);
            html += `<p class="explainer-text">Minor M<sub>11</sub> (remove row 1, column 1):</p>
                    <div class="matrix-display-container">${mtx.formatToHTML(minor11)}</div>
                    <div class="formula-box">C<sub>11</sub> = (-1)<sup>1+1</sup> &middot; det(M<sub>11</sub>) = 1 &middot; ${mtx.smartFormat(det_minor11)} = ${mtx.smartFormat(cofactor11)}</div>
                    <h3>Final Cofactor Matrix</h3>
                    <p class="explainer-text">Repeating this process for all elements yields:</p>`;
            const cofactorM = mtx.cofactorMatrix(m);
            html += `<div class="matrix-display-container">${mtx.formatToHTML(cofactorM)}</div>`;
            html += '</div>';
            return html;
        }

        function getMatrixTraceExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Matrix Trace</h3>
                    <p class="explainer-text">The trace of a square matrix is the sum of the elements on its main diagonal (from the top left to the bottom right).</p>
                    <div class="matrix-display-container">${mtx.formatToHTML(m)}</div>`;
            const trace = mtx.trace(m);
            const traceCalculation = m.map((row, i) => mtx.smartFormat(row[i])).join(' + ');
            html += `<div class="formula-box">tr(A) = ${traceCalculation} = ${mtx.smartFormat(trace)}</div>`;
            html += '</div>';
            return html;
        }
        
        function getRuleOfSarrusExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Rule of Sarrus (3x3 Determinant)</h3>
                    <p class="explainer-text">The Rule of Sarrus is a visual shortcut for calculating the determinant of a 3x3 matrix. We repeat the first two columns to the right of the matrix.</p>
                    <h3>Step 1: Augment the Matrix</h3>
                    <div class="formula-box" style="font-size: 1.5rem;">
                        ${m[0][0]} &nbsp; ${m[0][1]} &nbsp; ${m[0][2]} | ${m[0][0]} &nbsp; ${m[0][1]}<br>
                        ${m[1][0]} &nbsp; ${m[1][1]} &nbsp; ${m[1][2]} | ${m[1][0]} &nbsp; ${m[1][1]}<br>
                        ${m[2][0]} &nbsp; ${m[2][1]} &nbsp; ${m[2][2]} | ${m[2][0]} &nbsp; ${m[2][1]}
                    </div>
                    <h3>Step 2: Sum the products of the down-right diagonals</h3>`;
            const d1 = m[0][0] * m[1][1] * m[2][2];
            const d2 = m[0][1] * m[1][2] * m[2][0];
            const d3 = m[0][2] * m[1][0] * m[2][1];
            const sum_down = d1 + d2 + d3;
            html += `<div class="formula-box">(${m[0][0]}*${m[1][1]}*${m[2][2]}) + (${m[0][1]}*${m[1][2]}*${m[2][0]}) + (${m[0][2]}*${m[1][0]}*${m[2][1]}) = ${mtx.smartFormat(sum_down)}</div>
                    <h3>Step 3: Subtract the products of the up-right diagonals</h3>`;
            const u1 = m[2][0] * m[1][1] * m[0][2];
            const u2 = m[2][1] * m[1][2] * m[0][0];
            const u3 = m[2][2] * m[1][0] * m[0][1];
            const sum_up = u1 + u2 + u3;
            html += `<div class="formula-box">(${m[2][0]}*${m[1][1]}*${m[0][2]}) + (${m[2][1]}*${m[1][2]}*${m[0][0]}) + (${m[2][2]}*${m[1][0]}*${m[0][1]}) = ${mtx.smartFormat(sum_up)}</div>
                    <h3>Step 4: Final Determinant</h3>
                    <div class="formula-box">${mtx.smartFormat(sum_down)} - ${mtx.smartFormat(sum_up)} = ${mtx.smartFormat(sum_down - sum_up)}</div>`;
            html += '</div>';
            return html;
        }

        function getPermutationExpansionExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Determinant by Permutation Expansion</h3>
                    <p class="explainer-text">The Leibniz formula defines the determinant by summing over all permutations of the column indices. The formula is: det(A) = &Sigma; (sgn(&sigma;) &Pi; a<sub>i, &sigma;(i)</sub>)</p>`;
            const det = mtx.determinant(m);
            const n = m.length;

            if (n > 3) {
                html += `<p class="explainer-text mt-8">Showing the full permutation expansion for matrices larger than 3x3 is very lengthy. A 4x4 matrix has 24 permutations, and a 5x5 has 120. The final result is calculated below.</p>`;
            } else {
                const perms = mtx.permutations(n);
                html += `<h3>Step 1: List all permutations and their signs</h3>`;
                perms.forEach(p => {
                    const sign = mtx.permutationSign(p);
                    html += `<div class="formula-box">Permutation: [${p.map(i=>i+1).join(', ')}], Sign: ${sign}</div>`;
                });
                html += `<h3>Step 2: Calculate the signed product for each permutation</h3>`;
                let terms = [];
                perms.forEach(p => {
                    const sign = mtx.permutationSign(p);
                    let product = sign;
                    let termStrings = [];
                    for(let i=0; i<n; i++) {
                        product *= m[i][p[i]];
                        termStrings.push(mtx.smartFormat(m[i][p[i]]));
                    }
                    terms.push(product);
                    html += `<div class="formula-box">(${sign}) &middot; ${termStrings.join(' &middot; ')} = ${mtx.smartFormat(product)}</div>`;
                });
                html += `<h3>Step 3: Sum the terms</h3>
                        <div class="formula-box">${terms.map(t => `(${mtx.smartFormat(t)})`).join(' + ')}</div>`;
            }
            html += `<h3>Final Determinant</h3>
                    <div class="formula-box">det(A) = ${mtx.smartFormat(det)}</div>`;
            html += '</div>';
            return html;
        }

        function getHadamardProductExplanation(m1, m2) {
            let html = '<div class="explainer-container">';
            html += `<h3>Hadamard Product (Element-wise)</h3>
                    <p class="explainer-text">The Hadamard product (A &#8728; B) is a simple, entry-by-entry multiplication of two matrices of the same size. The formula is (A &#8728; B)<sub>ij</sub> = A<sub>ij</sub> * B<sub>ij</sub>.</p>
                    <div class="matrix-display-container">${mtx.formatToHTML(m1)}<span class="operator">&#8728;</span>${mtx.formatToHTML(m2)}</div>
                    <h3>Step-by-Step Calculation</h3>`;
            const result = mtx.hadamardProduct(m1, m2);
            html += `<div class="matrix-display-container">${mtx.formatToHTMLWithOps(m1, m2, '&middot;')}</div>
                    <h3>Final Result</h3>
                    <div class="matrix-display-container">${mtx.formatToHTML(result)}</div>`;
            html += '</div>';
            return html;
        }

        function getKroneckerProductExplanation(m1, m2) {
            let html = '<div class="explainer-container">';
            html += `<h3>Kronecker Product</h3>
                    <p class="explainer-text">The Kronecker product (A &otimes; B) creates a larger block matrix by multiplying each entry of matrix A by the entire matrix B.</p>
                    <h3>Setup</h3>
                    <div class="matrix-display-container">A = ${mtx.formatToHTML(m1)}<span class="operator">,</span> B = ${mtx.formatToHTML(m2)}</div>
                    <h3>Process</h3>
                    <p class="explainer-text">For example, the top-left block of the result is the top-left element of A multiplied by B:</p>
                    <div class="matrix-display-container">${mtx.smartFormat(m1[0][0])}<span class="operator">&middot;</span>${mtx.formatToHTML(m2)}<span class="operator">=</span>${mtx.formatToHTML(mtx.scalarMultiply(m2, m1[0][0]))}</div>
                    <h3>Final Result (A &otimes; B)</h3>`;
            const result = mtx.kroneckerProduct(m1, m2);
            html += `<div class="matrix-display-container">${mtx.formatToHTML(result)}</div>`;
            html += '</div>';
            return html;
        }

        function getMatrixNormsExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Matrix Norms</h3>
                    <p class="explainer-text">Matrix norms measure the "size" or "magnitude" of a matrix. Different norms capture different properties of the matrix's behavior.</p>
                    <h3>Frobenius Norm</h3>
                    <p class="explainer-text">The Frobenius norm is the square root of the sum of the absolute squares of its elements, similar to the standard vector norm.</p>`;
            const frobenius = mtx.frobeniusNorm(m);
            const frobeniusCalc = m.flat().map(v => `${v}<sup>2</sup>`).join(' + ');
            html += `<div class="formula-box">||A||<sub>F</sub> = &radic;(&Sigma; |a<sub>ij</sub>|<sup>2</sup>) = &radic;(${frobeniusCalc}) = ${mtx.smartFormat(frobenius)}</div>
                    <h3>Infinity Norm (Max Row Sum)</h3>
                    <p class="explainer-text">The infinity norm is the maximum absolute row sum of the matrix.</p>
                    <div class="formula-box">||A||<sub>&infin;</sub> = ${mtx.infinityNorm(m)}</div>
                    <h3>1-Norm (Max Column Sum)</h3>
                    <p class="explainer-text">The 1-norm is the maximum absolute column sum of the matrix.</p>
                    <div class="formula-box">||A||<sub>1</sub> = ${mtx.oneNorm(m)}</div>`;
            html += '</div>';
            return html;
        }

        function getPrincipalMinorsExplanation(m) {
            let html = '<div class="explainer-container">';
            html += `<h3>Principal Minors</h3>
                    <p class="explainer-text">The principal minors of a square matrix are the determinants of its leading principal submatrices (the square submatrices in the top-left corner).</p>`;
            const minors = mtx.principalMinors(m);
            minors.forEach((minor, k) => {
                html += `<h3>k = ${k+1} (D<sub>${k+1}</sub>)</h3>
                        <p class="explainer-text">The ${k+1}x${k+1} leading principal submatrix is:</p>`;
                const submatrix = m.slice(0,k+1).map(r => r.slice(0,k+1));
                html += `<div class="matrix-display-container">${mtx.formatToHTML(submatrix)}</div>
                        <div class="formula-box">Minor D<sub>${k+1}</sub> = det(A<sub>${k+1}</sub>) = ${mtx.smartFormat(minor)}</div>`;
            });
            html += '</div>';
            return html;
        }

        function getPermutationMatrixOpsExplanation(m, p) {
            let html = '<div class="explainer-container">';
            html += `<h3>Permutation Matrix Operations</h3>
                    <p class="explainer-text">Multiplying by a permutation matrix P reorders the rows (PA) or columns (AP) of a matrix A. A valid permutation matrix is a square matrix with exactly one '1' in each row and column, and '0's everywhere else.</p>
                    
                    <h3>Row Permutation (PA)</h3>
                    <p class="explainer-text">Left-multiplying by P (calculating PA) swaps the rows of A. The position of the '1' in each row of P determines which row from A is chosen for the new matrix.</p>`;
            
            const pa = mtx.multiply(p, m);

            // Detailed row explanation
            for(let i=0; i < p.length; i++) {
                const p_row = p[i];
                const j = p_row.indexOf(1);
                if (j !== -1) {
                    html += `<p class="explainer-text mt-4">For <strong>Row ${i+1}</strong> of the result: the '1' in P is in column ${j+1}. This selects <strong>Row ${j+1}</strong> from A.</p>`;
                }
            }

            html += `<div class="matrix-display-container">${mtx.formatToHTML(p)}<span class="operator">&middot;</span>${mtx.formatToHTML(m)}<span class="operator">=</span>${mtx.formatToHTML(pa)}</div>
                    
                    <h3>Column Permutation (AP)</h3>
                    <p class="explainer-text">Right-multiplying by P (calculating AP) swaps the columns of A. The position of the '1' in each column of P determines which column from A is chosen.</p>`;
            
            const ap = mtx.multiply(m, p);
            const pT = mtx.transpose(p);

            // Detailed column explanation
            for(let j=0; j < pT.length; j++) {
                const p_col = pT[j];
                const i = p_col.indexOf(1);
                if (i !== -1) {
                    html += `<p class="explainer-text mt-4">For <strong>Column ${j+1}</strong> of the result: the '1' in P is in row ${i+1}. This selects <strong>Column ${i+1}</strong> from A.</p>`;
                }
            }
            
            html += `<div class="matrix-display-container">${mtx.formatToHTML(m)}<span class="operator">&middot;</span>${mtx.formatToHTML(p)}<span class="operator">=</span>${mtx.formatToHTML(ap)}</div>`;
            html += '</div>';
            return html;
        }

        function initializeApp() {
        const tabsContainer = document.getElementById('operation-tabs');
        const calculatorBody = document.getElementById('calculator-body');
        document.getElementById('currentYear').textContent = new Date().getFullYear();
        const orderedOperations = Object.keys(operationsData);
        tabsContainer.innerHTML = orderedOperations.map(id => `<button class="tab-button" data-operation-id="${id}">${operationsData[id].name}</button>`).join('');
        
        const handleTabClick = (operationId) => {
            const operation = operationsData[operationId];
            tabsContainer.querySelectorAll('.tab-button').forEach(btn => btn.classList.toggle('active', btn.dataset.operationId === operationId));
            calculatorBody.innerHTML = '';
            if (chartInstance) { chartInstance.destroy(); chartInstance = null; }
            buildCalculatorUI(operation);
        };

        const generateInputGrids = (container, operation, sizeValue, dimValue) => {
                container.innerHTML = '';
                if (!sizeValue) return;
                if(operation.dimSizes && !dimValue) return;

                let html = '';
                const inputs = operation.inputs;

                for (const input of inputs) {
                    let gridHTML = '';
                    if (input.type === 'scalar') { gridHTML = `<div class="text-center"><h3 class="text-xl font-semibold mb-4 text-gray-200">${input.name}</h3><input type="number" id="scalar-input" class="scalar-input" placeholder="0"></div>`; }
                    else if (input.type === 'function') {
                        const placeholder = sizeValue === 'f(x, y)' ? 'e.g., x^2 + 2*x*y + y^2' : 'e.g., x*y*z';
                        gridHTML = `<div class="text-center">
                                        <h3 class="text-xl font-semibold mb-4 text-gray-200">${input.name}</h3>
                                        <input type="text" id="function-input" class="scalar-input" style="width: 20rem; text-align: left; padding-left: 1rem;" placeholder="${placeholder}">
                                    </div>`;
                    }
                    else if (input.type === 'permutation') {
                        gridHTML = `<div class="text-center"><h3 class="text-xl font-semibold mb-4 text-gray-200">${input.name}</h3><input type="text" id="permutation-input" class="scalar-input" style="width: 10rem;" placeholder="2,1,3"></div>`;
                    }
                    else if (input.type.startsWith('multivector')) {
                        const numVectors = parseInt(sizeValue);
                        let dim;
                        if (operation.id === 'basis-dimension') { dim = parseInt(sizeValue); }
                        else if (input.dim === 'any') { dim = parseInt(dimValue); }
                        else { dim = input.dim; }

                        let baseName = input.name.split(' ')[0];
                        if (baseName.toLowerCase() === 'set' || baseName.toLowerCase() === 'vectors') baseName = 'Vector';

                        for(let i=1; i <= numVectors; i++) {
                            const vecName = `${baseName} ${i}`;
                            gridHTML += `<div class="text-center"><h3 class="text-xl font-semibold mb-4 text-gray-200">${vecName}</h3>`;
                            gridHTML += `<div class="matrix-grid inline-grid" style="grid-template-columns: 1fr;">`;
                            for(let r=0; r < dim; r++) gridHTML += `<input type="number" class="matrix-input" placeholder="0" data-row="${r}" data-col="0" data-matrix="${vecName}">`;
                            gridHTML += `</div></div>`;
                            if(input.type === 'multivector_scalar') {
                                gridHTML += `<div class="text-center"><h3 class="text-xl font-semibold mb-4 text-gray-200">Scalar ${i}</h3><input type="number" id="scalar-input-${i}" class="scalar-input" placeholder="0"></div>`;
                            }
                        }
                        if(input.type === 'multivector_target') {
                            gridHTML += `<div class="text-center"><h3 class="text-xl font-semibold mb-4 text-gray-200">Target Vector</h3>`;
                            gridHTML += `<div class="matrix-grid inline-grid" style="grid-template-columns: 1fr;">`;
                            for(let r=0; r < dim; r++) gridHTML += `<input type="number" class="matrix-input" placeholder="0" data-row="${r}" data-col="0" data-matrix="Target Vector">`;
                            gridHTML += `</div></div>`;
                        }
                    } else {
                        let rows, cols;
                        if (input.type === 'vector') {
                            cols = 1;
                            if(operation.id.match(/augment-matrix|solve-axb|least-squares|change-of-basis|cramers-rule|kronecker-product/)) { 
                                const parts = sizeValue.split(' & ');
                                const dims = (input.name.includes('A') || input.name.includes('P')) ? parts[0].split('x') : parts[1].split('x');
                                rows = parseInt(dims[0]); // Use first part of dims for vector length
                            }
                            else { rows = parseInt(sizeValue); }
                        } else if (operation.id === 'matrix-multiplication') {
                            const parts = sizeValue.split(' * ');
                            const dims = (input.name === 'Matrix A') ? parts[0].split('x') : parts[1].split('x');
                            [rows, cols] = dims.map(Number);
                        } else if (operation.id.match(/augment-matrix|solve-axb|least-squares|change-of-basis|cramers-rule|kronecker-product/)) {
                            const parts = sizeValue.split(' & ');
                            const dims = (input.name.includes('A') || input.name.includes('P')) ? parts[0].split('x') : parts[1].split('x');
                            [rows, cols] = dims.map(Number);
                        } else { [rows, cols] = sizeValue.split('x').map(Number); }
                        
                        gridHTML += `<div class="text-center"><h3 class="text-xl font-semibold mb-4 text-gray-200">${input.name}</h3><div class="matrix-grid inline-grid" style="grid-template-columns: repeat(${cols}, 1fr);">`;
                        for (let i = 0; i < rows; i++) for (let j = 0; j < cols; j++) gridHTML += `<input type="number" class="matrix-input" placeholder="0" data-row="${i}" data-col="${j}" data-matrix="${input.name}">`;
                        gridHTML += `</div></div>`;
                    }
                    html += gridHTML;
                }
                container.innerHTML = html;
            };

        const buildCalculatorUI = (operation) => {
            const overviewHTML = `<details class="overview-details">
                                    <summary class="overview-summary">What is ${operation.name}?</summary>
                                    <div class="overview-content">${operation.overview.replace(/\n/g, '<br><br>')}</div>
                                </details>`;

            if (!operation.implemented) {
                calculatorBody.innerHTML = overviewHTML + `<div class="text-center py-10"><h2 class="text-3xl font-bold text-gray-300">Coming Soon...</h2></div>`;
                return;
            }
            
            let sizeOptions = operation.sizes.map(size => `<option value="${size.text}">${size.text}</option>`).join('');
            let dimOptions = operation.dimSizes ? operation.dimSizes.map(dim => `<option value="${dim.text}">${dim.text}</option>`).join('') : '';

            let configHTML = `<div class="control-label">Configuration</div><div class="flex flex-wrap justify-center gap-4">`;
            configHTML += `<select id="size-selector" class="styled-select"><option value="" disabled selected>${operation.dimSizes ? 'Num Vectors...' : 'Select Size...'}</option>${sizeOptions}</select>`;
            if (dimOptions) {
                configHTML += `<select id="dim-selector" class="styled-select"><option value="" disabled selected>Select Dim...</option>${dimOptions}</select>`;
            }
            configHTML += `</div>`;
            
            calculatorBody.innerHTML = `
                ${overviewHTML}
                <div class="p-4 mb-6 rounded-lg min-h-[70px] control-panel">${configHTML}</div>
                <div id="input-grids" class="flex justify-center gap-8 items-start flex-wrap"></div>
                <div class="text-center mt-8"><button id="calculate-btn" class="bg-blue-600 hover:bg-red-600 text-white font-bold py-3 px-8 rounded-lg text-lg">Calculate</button></div>
                <div id="results-area" class="mt-8"></div>`;

            const sizeSelector = calculatorBody.querySelector('#size-selector');
            const dimSelector = calculatorBody.querySelector('#dim-selector');
            const inputGrids = calculatorBody.querySelector('#input-grids');
            
            const updateGrids = () => {
                const sizeVal = sizeSelector.value;
                const dimVal = dimSelector ? dimSelector.value : null;
                generateInputGrids(inputGrids, operation, sizeVal, dimVal);
            };

            sizeSelector.addEventListener('change', updateGrids);
            if (dimSelector) dimSelector.addEventListener('change', updateGrids);

            calculatorBody.querySelector('#calculate-btn').addEventListener('click', () => {
                const resultsArea = calculatorBody.querySelector('#results-area');
                if (chartInstance) { chartInstance.destroy(); chartInstance = null; }
                let result;
                
                const opId = operation.id;
                try {
                    if (opId === 'identity-inverse') { result = getInverseExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'matrix-add-subtract') { result = getAdditionSubtractionExplanation(mtx.get('Matrix A'), mtx.get('Matrix B')); }
                    else if (opId === 'scalar-mult-matrix') { result = getScalarMultExplanation(mtx.get('Matrix A'), mtx.getScalar()); }
                    else if (opId === 'transpose') { result = getTransposeExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'symmetric-matrix') { result = getSymmetricCheckExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'matrix-multiplication') { result = getMatrixMultExplanation(mtx.get('Matrix A'), mtx.get('Matrix B')); }
                    else if (opId === 'augment-matrix') { result = getAugmentExplanation(mtx.get('Matrix A'), mtx.get('Vector b', true)); }
                    else if (opId === 'vector-add-subtract') { result = getVectorAddSubtractExplanation(mtx.get('Vector u', true), mtx.get('Vector v', true)); }
                    else if (opId === 'scalar-mult-vector') { result = getScalarVectorMultExplanation(mtx.get('Vector u', true), mtx.getScalar()); }
                    else if (opId === 'dot-product') { result = getDotProductExplanation(mtx.get('Vector u', true), mtx.get('Vector v', true)); }
                    else if (opId === 'cross-product') { result = getCrossProductExplanation(mtx.get('Vector u', true), mtx.get('Vector v', true)); }
                    else if (opId === 'vector-length-norm') { result = getNormExplanation(mtx.get('Vector u', true)); }
                    else if (opId === 'unit-vector') { result = getUnitVectorExplanation(mtx.get('Vector u', true)); }
                    else if (opId === 'angle-between-vectors') { result = getAngleBetweenExplanation(mtx.get('Vector u', true), mtx.get('Vector v', true)); }
                    else if (opId === 'vector-ops') { result = getVectorOpsExplanation(mtx.get('Vector u', true), mtx.get('Vector v', true)); }
                    else if (opId === '2d-vector-plotter') { const numVectors = parseInt(sizeSelector.value); result = get2DPlotterExplanation(mtx.getMultiple('Vector', numVectors, true)); }
                    else if (opId === 'linear-combination') { const numVectors = parseInt(sizeSelector.value); const vectors = mtx.getMultiple('Vector', numVectors, true); const scalars = mtx.getScalars('Scalar', numVectors); result = getLinearCombinationExplanation(vectors, scalars); }
                    else if (opId === 'span') { const numVectors = parseInt(sizeSelector.value); const vectors = mtx.getMultiple('Vector', numVectors, true); const target = mtx.get('Target Vector', true); result = getSpanExplanation(vectors, target); }
                    else if (opId === 'linear-independence') { const numVectors = parseInt(sizeSelector.value); const vectors = mtx.getMultiple('Vector', numVectors, true); result = getLinearIndependenceExplanation(vectors); }
                    else if (opId === 'basis-dimension') { const dim = parseInt(sizeSelector.value); result = getBasisDimensionExplanation(mtx.getMultiple('Vector', dim, true), dim); }
                    else if (opId === 'orthogonal-vectors') { result = getOrthogonalVectorsExplanation(mtx.get('Vector u', true), mtx.get('Vector v', true)); }
                    else if (opId === 'orthogonal-projection') { result = getOrthogonalProjectionExplanation(mtx.get('Vector u (to project)', true), mtx.get('Vector v (project onto)', true)); }
                    else if (opId === 'orthogonal-complement') { const numVectors = parseInt(sizeSelector.value); const vectors = mtx.getMultiple('Vector', numVectors, true); result = getOrthogonalComplementExplanation(vectors); }
                    else if (opId === 'gram-schmidt') { const numVectors = parseInt(sizeSelector.value); result = getGramSchmidtExplanation(mtx.getMultiple('Vector', numVectors, true)); }
                    else if (opId === 'ref') { result = getRefExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'step-by-step-rref') { result = getRrefStepsExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'determinant-rank') { result = getDeterminantRankExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'solve-axb') { result = getSolveAxbExplanation(mtx.get('Matrix A'), mtx.get('Vector b', true)); }
                    else if (opId === 'least-squares') { result = getLeastSquaresExplanation(mtx.get('Matrix A'), mtx.get('Vector b', true)); }
                    else if (opId === 'eigenvalues-eigenvectors') { result = getEigenExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'characteristic-equation') { result = getCharEqExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'diagonalization') { result = getDiagonalizationExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'svd') { result = getSvdExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'hessian-matrix') {
                        const funcStr = document.getElementById('function-input').value;
                        const vars = calculatorBody.querySelector('#size-selector').value === 'f(x, y)' ? ['x', 'y'] : ['x', 'y', 'z'];
                        result = getHessianExplanation(funcStr, vars);
                    }
                    else if (opId === 'lu-decomposition') { result = getLuDecompositionExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'qr-decomposition') { result = getQrDecompositionExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'cholesky-decomposition') { result = getCholeskyDecompositionExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'jordan-canonical-form') { result = getJordanCanonicalFormExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'minimal-polynomial') { result = getMinimalPolynomialExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'matrix-exponential') { result = getMatrixFunctionsExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'null-space') { result = getNullSpaceExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'column-row-space') { result = getColRowSpaceExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'image-rank-factorization') { result = getImageRankFactorizationExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'change-of-basis') { result = getChangeOfBasisExplanation(mtx.get('Matrix P (New to Old)'), mtx.get('Vector x (Old Basis)', true)); }
                    else if (opId === 'laplace-expansion') { result = getLaplaceExpansionExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'block-matrix-ops') { result = getBlockMatrixOpsExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'cramers-rule') { result = getCramersRuleExplanation(mtx.get('Matrix A'), mtx.get('Vector b', true)); }
                    else if (opId === 'adjugate-matrix') { result = getAdjugateMatrixExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'cofactor-matrix') { result = getCofactorMatrixExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'matrix-trace') { result = getMatrixTraceExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'rule-of-sarrus') { result = getRuleOfSarrusExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'permutation-expansion') { result = getPermutationExpansionExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'hadamard-product') { result = getHadamardProductExplanation(mtx.get('Matrix A'), mtx.get('Matrix B')); }
                    else if (opId === 'kronecker-product') { result = getKroneckerProductExplanation(mtx.get('Matrix A'), mtx.get('Matrix B')); }
                    else if (opId === 'matrix-norms') { result = getMatrixNormsExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'principal-minors') { result = getPrincipalMinorsExplanation(mtx.get('Matrix A')); }
                    else if (opId === 'permutation-matrix-ops') { result = getPermutationMatrixOpsExplanation(mtx.get('Matrix A'), mtx.get('Permutation Matrix P')); }

                    if (result && typeof result === 'object' && result.html) {
                        resultsArea.innerHTML = result.html;
                        if (result.onRender) result.onRender();
                    } else if (result) {
                        resultsArea.innerHTML = result;
                    }

                    if (window.renderMathInElement) renderMathInElement(resultsArea, { delimiters: [{left: '$$', right: '$$', display: true}] });
                } catch (e) {
                    console.error(e);
                    resultsArea.innerHTML = `<div class="explainer-container"><h3 style="color:var(--red-accent);">Error</h3><p class="explainer-text">An error occurred during calculation. Please check your inputs.</p><div class="formula-box" style="text-align:left; font-family: Inter, sans-serif;">${e.message}</div></div>`
                }
            });
        };
        
        const buildComingSoonUI = (operation) => {
            const overviewHTML = `<details class="overview-details">
                                    <summary class="overview-summary">What is ${operation.name}?</summary>
                                    <div class="overview-content">${operation.overview.replace(/\n/g, '<br><br>')}</div>
                                </details>`;
            calculatorBody.innerHTML = overviewHTML + `<div class="text-center py-10"><h2 class="text-3xl font-bold text-gray-300">Coming Soon...</h2></div>`;
        };
        
        tabsContainer.addEventListener('click', (e) => {
            if (e.target.matches('.tab-button')) handleTabClick(e.target.dataset.operationId);
        });

        // Set initial state to the first operation in the list
        const firstOpId = orderedOperations[0];
        if (firstOpId) handleTabClick(firstOpId);
    }

    document.addEventListener('DOMContentLoaded', initializeApp);
    })();
    </script>
    </body>
    </html>